"""
ë¶€ì‚°ì‹œì²­ ë³´ë„ìë£Œ ìš”ì•½ê¸° - BusanNewsSummarizer (ì‚­ì„  ë¬¸ì œ í•´ê²°)
================================================================
PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ â†’ GPTë¡œ ìš”ì•½ â†’ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ìƒì„±
"""

import os
import logging
import time
import re
from pathlib import Path
from typing import Dict, List, Optional
import openai
import fitz  # PyMuPDF
from datetime import datetime
from difflib import SequenceMatcher

from config import (
    OPENAI_API_KEY, OPENAI_MODEL, MAX_TOKENS, TEMPERATURE,
    AVAILABLE_TAGS, MD_DIR, PDF_DIR
)

logger = logging.getLogger(__name__)

class BusanNewsSummarizer:
    """ë¶€ì‚°ì‹œì²­ ë³´ë„ìë£Œ ìš”ì•½ê¸°"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        if not OPENAI_API_KEY:
            raise ValueError("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        self.client = openai.OpenAI(api_key=OPENAI_API_KEY)
        self.output_dir = MD_DIR
        Path(self.output_dir).mkdir(parents=True, exist_ok=True)
        logger.info("âœ… BusanNewsSummarizer ì´ˆê¸°í™” ì™„ë£Œ")
    
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        doc = None
        try:
            logger.info(f"ğŸ“„ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ: {Path(pdf_path).name}")
            
            if not os.path.exists(pdf_path) or os.path.getsize(pdf_path) < 100:
                logger.error("âŒ PDF íŒŒì¼ ë¬¸ì œ")
                return ""
            
            doc = fitz.open(pdf_path)
            if doc.page_count == 0:
                return ""
            
            text_parts = []
            for page_num in range(doc.page_count):
                try:
                    page_text = doc[page_num].get_text()
                    if page_text.strip():
                        text_parts.append(page_text)
                except Exception:
                    continue
            
            full_text = "\n".join(text_parts)
            
            # í…ìŠ¤íŠ¸ ì •ë¦¬
            if full_text:
                full_text = re.sub(r'\n\s*\n', '\n\n', full_text)
                full_text = re.sub(r'[ \t]+', ' ', full_text)
                full_text = re.sub(r'\n[ \t]+', '\n', full_text)
                full_text = re.sub(r'[^\w\sê°€-í£ã„±-ã…ã…-ã…£.,()[\]{}!?%-]', '', full_text)
                full_text = full_text.strip()
            
            if len(full_text) < 50:
                logger.warning("âš ï¸ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ")
                return ""
            
            logger.info(f"âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(full_text)}ì")
            return full_text
            
        except Exception as e:
            logger.error(f"âŒ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return ""
        finally:
            if doc:
                try:
                    doc.close()
                except:
                    pass

    def generate_summary_with_gpt(self, content: str, source_url: str = "") -> Optional[Dict]:
        """GPTë¥¼ ì´ìš©í•œ ìš”ì•½ ìƒì„±"""
        try:
            # ë‚´ìš© ê¸¸ì´ ì œí•œ
            if len(content) > 3000:
                content = content[:3000]
            
            # source_url ê¸°ë³¸ê°’
            if not source_url:
                source_url = "https://www.busan.go.kr/nbtnewsBU"
            
            # GPT í”„ë¡¬í”„íŠ¸
            prompt = f"""
ë‹¤ìŒ ë¶€ì‚°ì‹œì²­ ë³´ë„ìë£Œë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ìƒì„¸íˆ ìš”ì•½í•´ì£¼ì„¸ìš”.

**ì¤‘ìš” ì§€ì‹œì‚¬í•­:**
1. ë¬¸ì„œ ìƒë‹¨ì˜ ë‚ ì§œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš” (ì´ ë‚ ì§œë¥¼ date í•„ë“œì— ì‚¬ìš©í•˜ì„¸ìš”)
2. ë¬¸ì˜ì²˜ëŠ” PDF ì˜¤ë¥¸ìª½ ìƒë‹¨ í‘œì—ì„œ ì •í™•íˆ "ë‹´ë‹¹ì" í–‰ì˜ ì „í™”ë²ˆí˜¸ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
   - ê³¼ì¥ ë²ˆí˜¸ ì‚¬ìš© ê¸ˆì§€
   - íŒ€ì¥ ë²ˆí˜¸ ì‚¬ìš© ê¸ˆì§€  
   - ë°˜ë“œì‹œ "ë‹´ë‹¹ì" í–‰ì˜ ë²ˆí˜¸ë§Œ ì‚¬ìš©
   - ì˜ˆ: ë‹´ë‹¹ì ë³€ì¤€ì²  051-888-1382 â†’ ë¯¸ë””ì–´ë‹´ë‹¹ê´€ (051-888-1382)
   - ì˜ˆ: ë‹´ë‹¹ì ìµœì€ë½ 051-888-3742 â†’ ê³µê³µí•˜ìˆ˜ì¸í”„ë¼ê³¼ (051-888-3742)
3. ë‚ ì§œ ë²”ìœ„ í‘œê¸°ì‹œ "~" ëŒ€ì‹  "-" ë˜ëŠ” "ë¶€í„°"ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”
   - ì˜ëª»ëœ ì˜ˆ: 7ì›” 9ì¼~~30ì¼ (ë§ˆí¬ë‹¤ìš´ ì·¨ì†Œì„  ë¬¸ë²•)
   - ì˜¬ë°”ë¥¸ ì˜ˆ: 7ì›” 9ì¼-30ì¼ ë˜ëŠ” 7ì›” 9ì¼ë¶€í„° 30ì¼ê¹Œì§€
4. ë§ˆí¬ë‹¤ìš´ ë¬¸ë²•ì„ ì£¼ì˜í•´ì„œ ì‚¬ìš©í•˜ì„¸ìš”
   - "~~"ëŠ” ì·¨ì†Œì„  ë¬¸ë²•ì´ë¯€ë¡œ ë‚ ì§œ ë²”ìœ„ì— ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
   - ë‚ ì§œë‚˜ ì‹œê°„ ë²”ìœ„ëŠ” "-" ë˜ëŠ” "ë¶€í„°~ê¹Œì§€" í˜•íƒœë¡œ í‘œê¸°í•˜ì„¸ìš”

ë³´ë„ìë£Œ ë‚´ìš©:
{content}

ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:

---
title: "ë³´ë„ìë£Œ ì œëª©"
date: "YYYY-MM-DD"
tags: ["íƒœê·¸1"]
thumbnail_summary: "80ì ì´ë‚´ í•œì¤„ìš”ì•½"
source_url: "{source_url}"
---

# ìƒì„¸ ìš”ì•½

## ğŸ“‹ ì£¼ìš” ë‚´ìš©
ì´ë²ˆ ë³´ë„ìë£Œì˜ ëª¨ë“  ë‚´ìš©ì„ ìì—°ìŠ¤ëŸ½ê²Œ ìƒì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.

## ğŸ¯ í•µì‹¬ í¬ì¸íŠ¸
- ì£¼ìš” ëŒ€ìƒ, ì¼ì •, ì¥ì†Œ, ì°¸ì—¬ë°©ë²•, í˜œíƒ ë“±

## ğŸ“ ë¬¸ì˜ ë° ì‹ ì²­
í•´ë‹¹ ë¶€ì„œì™€ ë‹´ë‹¹ì ì „í™”ë²ˆí˜¸ë§Œ í‘œê¸°í•˜ì„¸ìš” (ì˜ˆ: ê³µê³µí•˜ìˆ˜ì¸í”„ë¼ê³¼ (051-888-3742))
ë°˜ë“œì‹œ PDF ì˜¤ë¥¸ìª½ ìƒë‹¨ "ë‹´ë‹¹ì" í–‰ì˜ ë²ˆí˜¸ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.

ì‚¬ìš© ê°€ëŠ¥í•œ íƒœê·¸: {', '.join(AVAILABLE_TAGS)}
íƒœê·¸ëŠ” 1ê°œë§Œ ì„ íƒí•´ì£¼ì„¸ìš”.
"""
            
            logger.info("ğŸ¤– GPT ìš”ì•½ ìƒì„± ì¤‘...")
            
            response = self.client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=[
                    {
                        "role": "system", 
                        "content": "ë¶€ì‚°ì‹œì²­ ë³´ë„ìë£Œ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. frontmatter í˜•ì‹ì„ ì •í™•íˆ ì§€ì¼œì£¼ì„¸ìš”. ë‚ ì§œ ë²”ìœ„ í‘œê¸°ì‹œ ë§ˆí¬ë‹¤ìš´ ì·¨ì†Œì„  ë¬¸ë²•(~~) ì‚¬ìš©ì„ í”¼í•´ì£¼ì„¸ìš”."
                    },
                    {"role": "user", "content": prompt}
                ],
                max_tokens=MAX_TOKENS,
                temperature=TEMPERATURE
            )
            
            summary_text = response.choices[0].message.content.strip()
            if not summary_text:
                return None
            
            # frontmatter íŒŒì‹±
            metadata = self._parse_frontmatter(summary_text)
            if not metadata:
                return None
            
            # ğŸ”§ ì•ˆì „í•œ source_url ì„¤ì • (ê°€ì¥ ë§ˆì§€ë§‰ì—)
            if not metadata.get('source_url') or metadata.get('source_url') == "ì›ë¬¸_URL_ì—¬ê¸°_ì…ë ¥":
                metadata['source_url'] = source_url
                summary_text = summary_text.replace("ì›ë¬¸_URL_ì—¬ê¸°_ì…ë ¥", source_url)
            
            # ğŸ”§ ë¬¸ì˜ì²˜ ì •ì œ (summary_text ìˆ˜ì • í›„ ë³„ë„ ì²˜ë¦¬)
            summary_text = self._clean_contact_info(summary_text)
            
            # ğŸ”§ ë§ˆí¬ë‹¤ìš´ ì·¨ì†Œì„  ë¬¸ë²• ì •ì œ
            summary_text = self._fix_markdown_strikethrough(summary_text)
            
            logger.info("âœ… GPT ìš”ì•½ ìƒì„± ì™„ë£Œ")
            return {
                'metadata': metadata,
                'content': summary_text
            }
                
        except Exception as e:
            logger.error(f"âŒ GPT ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _fix_markdown_strikethrough(self, text: str) -> str:
        """ğŸ”§ ë§ˆí¬ë‹¤ìš´ ì·¨ì†Œì„  ë¬¸ë²• ì •ì œ"""
        try:
            logger.debug("ğŸ”§ ë§ˆí¬ë‹¤ìš´ ì·¨ì†Œì„  ë¬¸ë²• ì •ì œ ì‹œì‘")
            
            # íŒ¨í„´ 1: ìˆ«ìì¼~~ìˆ«ìì¼ â†’ ìˆ«ìì¼-ìˆ«ìì¼
            text = re.sub(r'(\d+ì¼)~~(\d+ì¼)', r'\1-\2', text)
            
            # íŒ¨í„´ 2: ìˆ«ìì›” ìˆ«ìì¼~~ìˆ«ìì¼ â†’ ìˆ«ìì›” ìˆ«ìì¼-ìˆ«ìì¼
            text = re.sub(r'(\d+ì›”\s*\d+ì¼)~~(\d+ì¼)', r'\1-\2', text)
            
            # íŒ¨í„´ 3: ë…„.ì›”.ì¼~~ë…„.ì›”.ì¼ â†’ ë…„.ì›”.ì¼-ë…„.ì›”.ì¼
            text = re.sub(r'(\d+\.\d+\.\d+)~~(\d+\.\d+\.\d+)', r'\1-\2', text)
            
            # íŒ¨í„´ 4: ì‹œê°„~~ì‹œê°„ â†’ ì‹œê°„-ì‹œê°„
            text = re.sub(r'(\d+:\d+)~~(\d+:\d+)', r'\1-\2', text)
            
            # íŒ¨í„´ 5: ì¼ë°˜ì ì¸ ë²”ìœ„ í‘œê¸° A~~B â†’ A-B
            text = re.sub(r'([ê°€-í£\d\s]+)~~([ê°€-í£\d\s]+)', r'\1-\2', text)
            
            logger.debug("ğŸ”§ ë§ˆí¬ë‹¤ìš´ ì·¨ì†Œì„  ë¬¸ë²• ì •ì œ ì™„ë£Œ")
            return text
            
        except Exception as e:
            logger.error(f"âŒ ë§ˆí¬ë‹¤ìš´ ì·¨ì†Œì„  ë¬¸ë²• ì •ì œ ì‹¤íŒ¨: {e}")
            return text
    
    def _clean_contact_info(self, text: str) -> str:
        """ğŸ”§ ë¬¸ì˜ì²˜ ì •ì œ - GPTê°€ ì œëŒ€ë¡œ ì•ˆí–ˆì„ ë•Œ ë°±ì—… ì²˜ë¦¬"""
        try:
            logger.debug("ğŸ”§ ë¬¸ì˜ì²˜ ì •ì œ ì‹œì‘")
            
            # GPTê°€ ì´ë¯¸ ì˜¬ë°”ë¥¸ í˜•íƒœë¡œ ë§Œë“¤ì—ˆëŠ”ì§€ í™•ì¸
            # "ë¶€ì„œëª… (051-888-XXXX)" í˜•íƒœê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ë‘ê¸°
            if re.search(r'[ê°€-í£]{2,}(?:ê³¼|íŒ€|ì‹¤|êµ­|ë³¸ë¶€|ì„¼í„°)\s*\(051-\d{3}-\d{4}\)', text):
                logger.info("âœ… GPTê°€ ì˜¬ë°”ë¥¸ ë¬¸ì˜ì²˜ í˜•íƒœë¡œ ìƒì„±í•¨")
                return text
            
            # ê°œì¸ ì´ë¦„ì´ í¬í•¨ëœ ë¬¸ì˜ì²˜ íŒ¨í„´ ì°¾ì•„ì„œ ì •ì œ
            contact_patterns = [
                # "ê¹€ì˜êµ¬ ê³¼ì¥ (051-888-3730)" â†’ "ê³µê³µí•˜ìˆ˜ì¸í”„ë¼ê³¼ (051-888-3742)"
                r'([ê°€-í£]{2,4})\s*(ê³¼ì¥|íŒ€ì¥|ì£¼ì„|ì‚¬ë¬´ê´€|ë‹´ë‹¹ì?)\s*\(([^)]+)\)',
                # "ê³µê³µí•˜ìˆ˜ì¸í”„ë¼ê³¼ ê¹€ì˜êµ¬ ê³¼ì¥ (051-888-3730)" íŒ¨í„´
                r'([ê°€-í£]{2,}(?:ê³¼|íŒ€|ì‹¤|êµ­|ë³¸ë¶€|ì„¼í„°))\s*([ê°€-í£]{2,4})\s*(ê³¼ì¥|íŒ€ì¥|ì£¼ì„|ì‚¬ë¬´ê´€|ë‹´ë‹¹ì?)\s*\(([^)]+)\)',
            ]
            
            for pattern in contact_patterns:
                def clean_contact(match):
                    groups = match.groups()
                    if len(groups) == 3:  # ì´ë¦„ + ì§ì±… + ì „í™”ë²ˆí˜¸
                        return f"í•´ë‹¹ ë¶€ì„œ ({groups[2]})"
                    elif len(groups) == 4:  # ë¶€ì„œ + ì´ë¦„ + ì§ì±… + ì „í™”ë²ˆí˜¸
                        dept = groups[0]
                        phone = groups[3]
                        return f"{dept} ({phone})"
                    return match.group(0)
                
                text = re.sub(pattern, clean_contact, text)
            
            logger.debug("ğŸ”§ ë¬¸ì˜ì²˜ ì •ì œ ì™„ë£Œ")
            return text
            
        except Exception as e:
            logger.error(f"âŒ ë¬¸ì˜ì²˜ ì •ì œ ì‹¤íŒ¨: {e}")
            return text
    
    def _parse_frontmatter(self, content: str) -> Optional[Dict]:
        """frontmatter íŒŒì‹±"""
        try:
            if not content.startswith('---'):
                return None
            
            end_idx = content.find('---', 3)
            if end_idx == -1:
                return None
            
            frontmatter_text = content[3:end_idx].strip()
            metadata = {}
            
            for line in frontmatter_text.split('\n'):
                line = line.strip()
                if not line or ':' not in line:
                    continue
                
                key, value = line.split(':', 1)
                key = key.strip()
                value = value.strip().strip('"\'')
                
                if key == 'tags':
                    if value.startswith('[') and value.endswith(']'):
                        tag_content = value[1:-1].strip()
                        if tag_content:
                            tags = [tag.strip().strip('"\'') for tag in tag_content.split(',') if tag.strip()]
                            metadata[key] = tags if tags else ["ì „ì²´"]
                        else:
                            metadata[key] = ["ì „ì²´"]
                    else:
                        metadata[key] = [value] if value else ["ì „ì²´"]
                else:
                    metadata[key] = value
            
            # í•„ìˆ˜ í•„ë“œ ê¸°ë³¸ê°’
            if 'title' not in metadata:
                metadata['title'] = "ë¶€ì‚°ì‹œ ë³´ë„ìë£Œ"
            if 'date' not in metadata:
                metadata['date'] = datetime.now().strftime("%Y-%m-%d")
            if 'tags' not in metadata:
                metadata['tags'] = ["ì „ì²´"]
            if 'thumbnail_summary' not in metadata:
                metadata['thumbnail_summary'] = "ë¶€ì‚°ì‹œ ë³´ë„ìë£Œì…ë‹ˆë‹¤."
            
            return metadata
            
        except Exception as e:
            logger.error(f"âŒ frontmatter íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None
    
    def _enhance_tags(self, gpt_tags: List[str], title: str, content: str) -> List[str]:
        """íƒœê·¸ ê²€ì¦ ë° ê°œì„  (1ê°œë§Œ ì„ íƒ)"""
        try:
            text_for_analysis = f"{title} {content}".lower()
            
            # GPT íƒœê·¸ ì¤‘ ìœ íš¨í•œ ê²ƒ ì„ íƒ
            for tag in gpt_tags:
                if tag in AVAILABLE_TAGS:
                    return [tag]
            
            # í‚¤ì›Œë“œ ë§¤ì¹­
            tag_keywords = {
                "ì²­ë…„Â·êµìœ¡": ["ì²­ë…„", "êµìœ¡", "í•™êµ", "ëŒ€í•™", "í•™ìƒ"],
                "ì¼ìë¦¬Â·ê²½ì œ": ["ì¼ìë¦¬", "ê²½ì œ", "ê¸°ì—…", "ì‚°ì—…", "íˆ¬ì"],
                "ë³µì§€Â·ê±´ê°•": ["ë³µì§€", "ê±´ê°•", "ì˜ë£Œ", "ë³‘ì›", "ëŒë´„"],
                "êµí†µÂ·ì£¼ê±°": ["êµí†µ", "ì£¼ê±°", "ì£¼íƒ", "ë²„ìŠ¤", "ì§€í•˜ì² "],
                "ë¬¸í™”Â·ì—¬ê°€": ["ë¬¸í™”", "ì—¬ê°€", "ì¶•ì œ", "ê³µì—°", "ì „ì‹œ"],
                "ì•ˆì „Â·í™˜ê²½": ["ì•ˆì „", "í™˜ê²½", "í™”ì¬", "ì¬ë‚œ", "íê¸°ë¬¼"],
                "í–‰ì •Â·ì°¸ì—¬": ["í–‰ì •", "ì°¸ì—¬", "ì‹œë¯¼", "ì •ì±…", "íšŒì˜"],
                "ê´€ê´‘Â·ì†Œì‹": ["ê´€ê´‘", "ì†Œì‹", "ë°©ë¬¸", "í™ë³´", "êµ­ì œ"]
            }
            
            tag_scores = {}
            for tag, keywords in tag_keywords.items():
                score = sum(1 for keyword in keywords if keyword in text_for_analysis)
                if score > 0:
                    tag_scores[tag] = score
            
            if tag_scores:
                best_tag = max(tag_scores, key=tag_scores.get)
                return [best_tag]
            
            return ["í–‰ì •Â·ì°¸ì—¬"]  # ê¸°ë³¸ê°’
            
        except Exception as e:
            logger.error(f"âŒ íƒœê·¸ ê°œì„  ì‹¤íŒ¨: {e}")
            return ["ì „ì²´"]
    
    def create_filename_from_metadata(self, metadata: Dict) -> str:
        """ë©”íƒ€ë°ì´í„°ë¡œë¶€í„° íŒŒì¼ëª… ìƒì„±"""
        try:
            title = metadata.get('title', 'ë³´ë„ìë£Œ')
            date = metadata.get('date', datetime.now().strftime("%Y-%m-%d"))
            tags = metadata.get('tags', ['ì „ì²´'])
            
            clean_date = date.replace('-', '')
            main_tag = tags[0] if tags else 'ì „ì²´'
            clean_title = self._clean_title_for_filename(title)
            
            filename = f"{clean_date}_{main_tag}_{clean_title}.md"
            
            if len(filename) > 100:
                clean_title = clean_title[:50]
                filename = f"{clean_date}_{main_tag}_{clean_title}.md"
            
            return filename
            
        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ëª… ìƒì„± ì‹¤íŒ¨: {e}")
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            return f"{timestamp}_ë³´ë„ìë£Œ.md"
    
    def _clean_title_for_filename(self, title: str) -> str:
        """íŒŒì¼ëª…ìš© ì œëª© ì •ë¦¬"""
        try:
            clean_title = re.sub(r'[<>:"/\\|?*\[\]{}]', '', title)
            clean_title = re.sub(r'\s+', '_', clean_title.strip())
            clean_title = re.sub(r'_+', '_', clean_title)
            
            # ì¡°ì‚¬ ì œê±° ë° ë‹¨ì–´ ì œí•œ
            particles = ['ì˜', 'ì„', 'ë¥¼', 'ì—', 'ì„œ', 'ë¡œ', 'ìœ¼ë¡œ', 'ì™€', 'ê³¼', 'ì´', 'ê°€', 'ëŠ”', 'ë„']
            words = [w for w in clean_title.split('_') if w and w not in particles and len(w) > 1]
            
            if len(words) > 4:
                words = words[:4]
            
            clean_title = '_'.join(words)
            if len(clean_title) > 30:
                clean_title = clean_title[:30]
            
            return clean_title.strip('_') if clean_title else "ë³´ë„ìë£Œ"
            
        except Exception as e:
            return "ë³´ë„ìë£Œ"
    
    def save_markdown(self, summary_data: Dict, filename: str) -> str:
        """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì €ì¥"""
        try:
            filepath = Path(self.output_dir) / filename
            
            if filepath.exists():
                backup_path = filepath.with_suffix('.md.bak')
                filepath.rename(backup_path)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(summary_data['content'])
            
            logger.info(f"âœ… ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì €ì¥: {filename}")
            return str(filepath)
            
        except Exception as e:
            logger.error(f"âŒ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return ""
    
    def _is_duplicate_pdf(self, pdf_path: str) -> Optional[str]:
        """PDF ì¤‘ë³µ í™•ì¸ (ê°„ì†Œí™”)"""
        try:
            pdf_filename = Path(pdf_path).name
            pdf_base = pdf_filename.replace('.pdf', '')
            pdf_normalized = re.sub(r'[^\wê°€-í£]', '', pdf_base.lower())
            
            md_files = list(Path(self.output_dir).glob("*.md"))
            
            for md_file in md_files:
                md_normalized = re.sub(r'[^\wê°€-í£]', '', md_file.stem.lower())
                similarity = SequenceMatcher(None, pdf_normalized, md_normalized).ratio()
                
                if similarity >= 0.8:
                    logger.info(f"ğŸ” ì¤‘ë³µ íŒŒì¼ ë°œê²¬: {pdf_filename} â‰ˆ {md_file.name}")
                    return str(md_file)
            
            return None
            
        except Exception as e:
            logger.error(f"âŒ ì¤‘ë³µ í™•ì¸ ì‹¤íŒ¨: {e}")
            return None
    
    def process_pdf_file(self, pdf_path: str, source_url: str = "") -> Optional[str]:
        """PDF íŒŒì¼ ì²˜ë¦¬"""
        try:
            pdf_filename = Path(pdf_path).name
            logger.info(f"ğŸš€ PDF ì²˜ë¦¬: {pdf_filename}")
            
            # ì¤‘ë³µ í™•ì¸
            existing_md = self._is_duplicate_pdf(pdf_path)
            if existing_md:
                logger.info(f"â­ï¸ ì¤‘ë³µ ìŠ¤í‚µ: {pdf_filename}")
                return existing_md
            
            # ê¸°ë³¸ URL ì„¤ì •
            if not source_url:
                source_url = "https://www.busan.go.kr/nbtnewsBU"
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            content = self.extract_text_from_pdf(pdf_path)
            if not content:
                return None
            
            # GPT ìš”ì•½
            summary_data = self.generate_summary_with_gpt(content, source_url)
            if not summary_data:
                return None
            
            # íƒœê·¸ ê°œì„ 
            metadata = summary_data['metadata']
            enhanced_tags = self._enhance_tags(
                metadata.get('tags', ['ì „ì²´']), 
                metadata.get('title', ''), 
                content
            )
            metadata['tags'] = enhanced_tags
            summary_data['metadata'] = metadata
            
            # íŒŒì¼ ì €ì¥
            filename = self.create_filename_from_metadata(metadata)
            md_path = self.save_markdown(summary_data, filename)
            
            if md_path:
                logger.info(f"ğŸ‰ ì™„ë£Œ: {pdf_filename} â†’ {filename}")
                return md_path
            
            return None
                
        except Exception as e:
            logger.error(f"âŒ PDF ì²˜ë¦¬ ì‹¤íŒ¨ {Path(pdf_path).name}: {e}")
            return None
    
    def process_all_pdfs(self, source_urls: Dict[str, str] = None) -> List[str]:
        """ëª¨ë“  PDF íŒŒì¼ ì²˜ë¦¬"""
        try:
            pdf_files = list(Path(PDF_DIR).glob("*.pdf"))
            if not pdf_files:
                logger.warning("âš ï¸ ì²˜ë¦¬í•  PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
                return []
            
            logger.info(f"ğŸ“‹ ì²˜ë¦¬í•  PDF: {len(pdf_files)}ê°œ")
            
            generated_files = []
            for idx, pdf_path in enumerate(pdf_files, 1):
                logger.info(f"ğŸ“„ [{idx}/{len(pdf_files)}] {pdf_path.name}")
                
                source_url = ""
                if source_urls and pdf_path.name in source_urls:
                    source_url = source_urls[pdf_path.name]
                
                md_file = self.process_pdf_file(str(pdf_path), source_url)
                if md_file:
                    generated_files.append(md_file)
                
                if idx < len(pdf_files):
                    time.sleep(2)  # API ì œí•œ
            
            logger.info(f"ğŸ‰ ì²˜ë¦¬ ì™„ë£Œ: {len(generated_files)}ê°œ ìƒì„±")
            return generated_files
            
        except Exception as e:
            logger.error(f"âŒ ì „ì²´ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return []