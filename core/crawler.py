import os
import time
import requests
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup, Tag
from urllib.parse import urljoin
from datetime import datetime
from typing import Optional
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class BusanNewsCrawler:
    def __init__(self, download_dir="./data/pdfs"):
        self.base_list_url = "https://www.busan.go.kr/nbtnewsBU?curPage={}"
        self.base_url = "https://www.busan.go.kr"
        self.download_dir = download_dir
        self.driver = None
        
        # ë‹¤ìš´ë¡œë“œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.download_dir, exist_ok=True)
        
    def setup_driver(self):
        """Selenium ë“œë¼ì´ë²„ ì„¤ì •"""
        logger.info("Selenium ë“œë¼ì´ë²„ ì¤€ë¹„ ì¤‘...")
        options = webdriver.ChromeOptions()
        options.add_argument('--headless')  # ì°½ ì•ˆ ë„ìš°ê³  ì‹¤í–‰
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920,1080')
        
        try:
            self.driver = webdriver.Chrome(
                service=Service(ChromeDriverManager().install()), 
                options=options
            )
            logger.info("ë“œë¼ì´ë²„ ì¤€ë¹„ ì™„ë£Œ")
            return True
        except Exception as e:
            logger.error(f"ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨: {e}")
            return False
    
    def get_news_links(self, max_pages=5):
        """ë³´ë„ìë£Œ ìƒì„¸ í˜ì´ì§€ ë§í¬ë“¤ì„ ìˆ˜ì§‘"""
        if not self.driver:
            logger.error("ë“œë¼ì´ë²„ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        all_links = []
        
        for page in range(1, max_pages + 1):
            logger.info(f'í˜ì´ì§€ {page} ìš”ì²­ ì¤‘...')
            try:
                self.driver.get(self.base_list_url.format(page))
                time.sleep(2)
                
                soup = BeautifulSoup(self.driver.page_source, "html.parser")
                news_links = soup.select("a.item")
                
                page_links = []
                for a_tag in news_links:
                    if isinstance(a_tag, Tag):
                        detail_href = a_tag.get("href")
                        if detail_href:
                            detail_url = urljoin(self.base_url, str(detail_href))
                            # ì œëª©ê³¼ ë‚ ì§œ í•¨ê»˜ ìˆ˜ì§‘
                            title_elem = a_tag.select_one(".subject")
                            title = title_elem.get_text(strip=True) if title_elem else "ì œëª©ì—†ìŒ"
                            
                            # ë‚ ì§œ ì •ë³´ ì¶”ì¶œ
                            date = self._extract_date_from_list_page(a_tag)
                            
                            page_links.append({
                                'url': detail_url,
                                'title': title,
                                'date': date,
                                'page': page
                            })
                
                logger.info(f'  - í˜ì´ì§€ {page}ì—ì„œ {len(page_links)}ê°œ ë§í¬ ë°œê²¬')
                all_links.extend(page_links)
                
            except Exception as e:
                logger.error(f"í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        logger.info(f"ì´ {len(all_links)}ê°œì˜ ë³´ë„ìë£Œ ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ")
        return all_links
    
    def download_pdfs_from_page(self, news_item):
        """íŠ¹ì • ë³´ë„ìë£Œ í˜ì´ì§€ì—ì„œ PDF ë‹¤ìš´ë¡œë“œ"""
        url = news_item['url']
        title = news_item['title']
        original_date = news_item.get('date')  # ëª©ë¡ì—ì„œ ê°€ì ¸ì˜¨ ë‚ ì§œ
        
        try:
            logger.info(f'ìƒì„¸í˜ì´ì§€ ì§„ì…: {title}')
            self.driver.get(url)
            time.sleep(1.5)
            
            detail_soup = BeautifulSoup(self.driver.page_source, "html.parser")
            
            # ìƒì„¸ í˜ì´ì§€ì—ì„œë„ ë‚ ì§œ ì¶”ì¶œ ì‹œë„ (ë” ì •í™•í•  ìˆ˜ ìˆìŒ)
            detail_date = self._extract_date_from_detail_page(detail_soup)
            final_date = detail_date if detail_date else original_date
            
            # PDF ì²¨ë¶€íŒŒì¼ ë§í¬ ì°¾ê¸°
            pdf_links = detail_soup.find_all("a", string=lambda t: bool(t and ".pdf" in t))
            
            if not pdf_links:
                logger.info(f'      - PDF ë§í¬ ì—†ìŒ: {title}')
                return []
            
            downloaded_files = []
            
            for pdf_link in pdf_links:
                if not isinstance(pdf_link, Tag):
                    continue
                    
                pdf_href = pdf_link.get('href')
                if not pdf_href:
                    continue
                
                pdf_url = urljoin(self.base_url, str(pdf_href))
                pdf_name = pdf_link.text.strip()
                
                # íŒŒì¼ëª… ì •ë¦¬ (íŠ¹ìˆ˜ë¬¸ì ì œê±°)
                safe_filename = self._clean_filename(pdf_name)
                if not safe_filename.endswith('.pdf'):
                    safe_filename += '.pdf'
                
                pdf_path = os.path.join(self.download_dir, safe_filename)
                
                # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” íŒŒì¼ ì²´í¬
                if os.path.exists(pdf_path):
                    logger.info(f"      - ì´ë¯¸ ì¡´ì¬: {safe_filename}")
                    downloaded_files.append({
                        'filename': safe_filename,
                        'path': pdf_path,
                        'title': title,
                        'date': final_date,  # ë‚ ì§œ ì •ë³´ ì¶”ê°€
                        'url': url,
                        'status': 'already_exists'
                    })
                    continue
                
                # PDF ë‹¤ìš´ë¡œë“œ
                try:
                    logger.info(f"      ğŸ“¥ ë‹¤ìš´ë¡œë“œ: {safe_filename}")
                    pdf_response = requests.get(pdf_url, timeout=30)
                    pdf_response.raise_for_status()
                    
                    with open(pdf_path, "wb") as f:
                        f.write(pdf_response.content)
                    
                    downloaded_files.append({
                        'filename': safe_filename,
                        'path': pdf_path,
                        'title': title,
                        'date': final_date,  # ë‚ ì§œ ì •ë³´ ì¶”ê°€
                        'url': url,
                        'status': 'downloaded',
                        'size': len(pdf_response.content)
                    })
                    
                    logger.info(f"      âœ… ì™„ë£Œ: {safe_filename} ({len(pdf_response.content)/1024:.1f}KB)")
                    time.sleep(0.5)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
                    
                except Exception as e:
                    logger.error(f"      âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {safe_filename} - {e}")
                    continue
            
            return downloaded_files
            
        except Exception as e:
            logger.error(f"í˜ì´ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {title} - {e}")
            return []
    
    def _extract_date_from_list_page(self, news_item_tag):
        """ë³´ë„ìë£Œ ëª©ë¡ í˜ì´ì§€ì—ì„œ ë‚ ì§œ ì¶”ì¶œ"""
        try:
            # ì—¬ëŸ¬ ê°€ëŠ¥í•œ ë‚ ì§œ ì„ íƒì ì‹œë„
            date_selectors = [
                ".date",
                ".reg_date", 
                ".write_date",
                ".board_date"
            ]
            
            for selector in date_selectors:
                date_elem = news_item_tag.select_one(selector)
                if date_elem:
                    date_text = date_elem.get_text(strip=True)
                    parsed_date = self._parse_date_string(date_text)
                    if parsed_date:
                        return parsed_date
            
            # ì„ íƒìë¡œ ì°¾ì§€ ëª»í•œ ê²½ìš°, í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ íŒ¨í„´ ì°¾ê¸°
            all_text = news_item_tag.get_text()
            parsed_date = self._parse_date_string(all_text)
            if parsed_date:
                return parsed_date
            
            # ë‚ ì§œë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° í˜„ì¬ ë‚ ì§œ ë°˜í™˜
            logger.warning("ë‚ ì§œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ í˜„ì¬ ë‚ ì§œ ì‚¬ìš©")
            return datetime.now().strftime("%Y-%m-%d")
            
        except Exception as e:
            logger.warning(f"ë‚ ì§œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return datetime.now().strftime("%Y-%m-%d")
    
    def _parse_date_string(self, date_text: str) -> Optional[str]:
        """ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ì„ YYYY-MM-DDë¡œ ë³€í™˜"""
        if not date_text:
            return None
        
        import re
        from datetime import datetime
        
        # ë‹¤ì–‘í•œ ë‚ ì§œ íŒ¨í„´ ì •ì˜
        date_patterns = [
            # 2025-07-04 í˜•ì‹
            (r'(\d{4})-(\d{1,2})-(\d{1,2})', r'\1-\2-\3'),
            # 2025.07.04 í˜•ì‹
            (r'(\d{4})\.(\d{1,2})\.(\d{1,2})', r'\1-\2-\3'),
            # 2025/07/04 í˜•ì‹
            (r'(\d{4})/(\d{1,2})/(\d{1,2})', r'\1-\2-\3'),
            # 2025ë…„ 7ì›” 4ì¼ í˜•ì‹
            (r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼', r'\1-\2-\3'),
            # 07-04 í˜•ì‹ (ë…„ë„ ì—†ìŒ, í˜„ì¬ ë…„ë„ ê°€ì •)
            (r'(\d{1,2})-(\d{1,2})(?!\d)', f'{datetime.now().year}-\\1-\\2'),
            # 07.04 í˜•ì‹ (ë…„ë„ ì—†ìŒ, í˜„ì¬ ë…„ë„ ê°€ì •)
            (r'(\d{1,2})\.(\d{1,2})(?!\d)', f'{datetime.now().year}-\\1-\\2'),
        ]
        
        for pattern, replacement in date_patterns:
            match = re.search(pattern, date_text)
            if match:
                try:
                    if len(pattern.split('(')) > 3:  # ë…„ì›”ì¼ì´ ëª¨ë‘ ìˆëŠ” ê²½ìš°
                        year = int(match.group(1))
                        month = int(match.group(2))
                        day = int(match.group(3))
                    else:  # ì›”ì¼ë§Œ ìˆëŠ” ê²½ìš°
                        year = datetime.now().year
                        month = int(match.group(1))
                        day = int(match.group(2))
                    
                    # ë‚ ì§œ ìœ íš¨ì„± ê²€ì‚¬
                    if 1 <= month <= 12 and 1 <= day <= 31:
                        date_obj = datetime(year, month, day)
                        return date_obj.strftime("%Y-%m-%d")
                except ValueError:
                    continue
        
        return None
    
    def _extract_date_from_detail_page(self, detail_soup) -> Optional[str]:
        """ìƒì„¸ í˜ì´ì§€ì—ì„œ ë‚ ì§œ ì •ë³´ ì¶”ì¶œ"""
        try:
            # ìƒì„¸ í˜ì´ì§€ì˜ ë‚ ì§œ ì„ íƒìë“¤
            date_selectors = [
                ".view_info .date",
                ".board_view .date", 
                ".news_info .date",
                ".article_info .date",
                "td:contains('ì‘ì„±ì¼')",
                "td:contains('ë“±ë¡ì¼')",
                ".reg_date",
                ".write_date"
            ]
            
            for selector in date_selectors:
                date_elem = detail_soup.select_one(selector)
                if date_elem:
                    date_text = date_elem.get_text(strip=True)
                    parsed_date = self._parse_date_string(date_text)
                    if parsed_date:
                        return parsed_date
            
            # í˜ì´ì§€ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ ì°¾ê¸°
            page_text = detail_soup.get_text()
            parsed_date = self._parse_date_string(page_text)
            if parsed_date:
                return parsed_date
            
            return None
            
        except Exception as e:
            logger.warning(f"ìƒì„¸ í˜ì´ì§€ ë‚ ì§œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def _clean_filename(self, filename: str) -> str:
        """íŒŒì¼ëª…ì—ì„œ íŠ¹ìˆ˜ë¬¸ì ì œê±° (ìœˆë„ìš° í˜¸í™˜)"""
        import re
        # ìœˆë„ìš°ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ë¬¸ìë“¤ ì œê±°
        cleaned = re.sub(r'[<>:"/\\|?*]', '_', filename)
        # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()
        return cleaned
    
    def crawl_news(self, max_pages=5):
        """ğŸ”§ ì „ì²´ í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ (ë©”ì„œë“œëª… ë³€ê²½)"""
        logger.info("ë¶€ì‚°ì‹œì²­ ë³´ë„ìë£Œ í¬ë¡¤ë§ ì‹œì‘!")
        
        # ë“œë¼ì´ë²„ ì„¤ì •
        if not self.setup_driver():
            logger.error("í¬ë¡¤ë§ì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        try:
            # 1. ë³´ë„ìë£Œ ë§í¬ ìˆ˜ì§‘
            news_links = self.get_news_links(max_pages)
            if not news_links:
                logger.warning("ìˆ˜ì§‘ëœ ë³´ë„ìë£Œ ë§í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            # 2. ê° í˜ì´ì§€ì—ì„œ PDF ë‹¤ìš´ë¡œë“œ
            all_downloaded = []
            total_links = len(news_links)
            
            for idx, news_item in enumerate(news_links, 1):
                logger.info(f"[{idx}/{total_links}] ì²˜ë¦¬ ì¤‘: {news_item['title'][:50]}...")
                downloaded_files = self.download_pdfs_from_page(news_item)
                all_downloaded.extend(downloaded_files)
            
            # 3. ê²°ê³¼ ìš”ì•½
            self._print_summary(all_downloaded)
            return all_downloaded
            
        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []
        finally:
            self.close()
    
    def _print_summary(self, downloaded_files):
        """í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        total = len(downloaded_files)
        new_downloads = len([f for f in downloaded_files if f.get('status') == 'downloaded'])
        existing = len([f for f in downloaded_files if f.get('status') == 'already_exists'])
        
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½")
        logger.info("="*60)
        logger.info(f"ì´ ì²˜ë¦¬ëœ íŒŒì¼: {total}ê°œ")
        logger.info(f"ìƒˆë¡œ ë‹¤ìš´ë¡œë“œ: {new_downloads}ê°œ")
        logger.info(f"ê¸°ì¡´ íŒŒì¼: {existing}ê°œ")
        logger.info(f"ì €ì¥ ìœ„ì¹˜: {self.download_dir}")
        logger.info("="*60)
        
        if new_downloads > 0:
            logger.info("ğŸ†• ìƒˆë¡œ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ë“¤:")
            for f in downloaded_files:
                if f.get('status') == 'downloaded':
                    size_kb = f.get('size', 0) / 1024
                    date_info = f"({f.get('date', 'N/A')})" if f.get('date') else ""
                    logger.info(f"  - {f['filename']} {date_info} ({size_kb:.1f}KB)")
    
    def get_recent_files(self, days=7):
        """ìµœê·¼ Nì¼ ë‚´ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ë“¤ ë°˜í™˜"""
        if not os.path.exists(self.download_dir):
            return []
        
        recent_files = []
        current_time = time.time()
        cutoff_time = current_time - (days * 24 * 60 * 60)
        
        for filename in os.listdir(self.download_dir):
            if filename.endswith('.pdf'):
                file_path = os.path.join(self.download_dir, filename)
                file_mtime = os.path.getmtime(file_path)
                
                if file_mtime >= cutoff_time:
                    recent_files.append({
                        'filename': filename,
                        'path': file_path,
                        'modified': datetime.fromtimestamp(file_mtime)
                    })
        
        # ìˆ˜ì • ì‹œê°„ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        recent_files.sort(key=lambda x: x['modified'], reverse=True)
        return recent_files
    
    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.driver:
            self.driver.quit()
            logger.info("ë“œë¼ì´ë²„ ì¢…ë£Œ ì™„ë£Œ")

def main():
    """í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì˜ˆì‹œ"""
    crawler = BusanNewsCrawler()
    
    # ìµœê·¼ 3í˜ì´ì§€ë§Œ í¬ë¡¤ë§ (í…ŒìŠ¤íŠ¸ìš©)
    downloaded_files = crawler.crawl_news(max_pages=3)
    
    # ìµœê·¼ 7ì¼ ë‚´ íŒŒì¼ë“¤ í™•ì¸
    recent = crawler.get_recent_files(days=7)
    if recent:
        print(f"\nğŸ“… ìµœê·¼ 7ì¼ ë‚´ íŒŒì¼ {len(recent)}ê°œ:")
        for f in recent[:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
            print(f"  - {f['filename']} ({f['modified'].strftime('%Y-%m-%d %H:%M')})")

if __name__ == "__main__":
    main()