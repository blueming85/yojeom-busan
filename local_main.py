"""
ë¶€ì‚°ì‹œì²­ ë³´ë„ìë£Œ í¬í„¸ - ë¡œì»¬ í†µí•© ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
=================================================
í¬ë¡¤ë§ â†’ ìš”ì•½ê¹Œì§€ ì›í´ë¦­ ì‹¤í–‰ (ì¸ë„¤ì¼ ì œê±°)

ì‚¬ìš©ë²•:
    python local_main.py                    # ì „ì²´ ì‹¤í–‰
    python local_main.py --crawl-only       # í¬ë¡¤ë§ë§Œ
    python local_main.py --summarize-only   # ìš”ì•½ë§Œ
    python local_main.py --test             # í…ŒìŠ¤íŠ¸ ëª¨ë“œ (2í˜ì´ì§€)
    python local_main.py --max-pages 5      # ìµœëŒ€ í˜ì´ì§€ ìˆ˜ ì§€ì •
"""

import os
import sys
import logging
import argparse
import traceback
from pathlib import Path
from typing import List, Dict, Optional

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent))

# ì»´í¬ë„ŒíŠ¸ import
try:
    from core.crawler import BusanNewsCrawler
    from core.summarizer import BusanNewsSummarizer
    from config import (
        LOG_FILE, PDF_DIR, MD_DIR,
        OPENAI_API_KEY, IS_LOCAL, IS_DEPLOYMENT
    )
except ImportError as e:
    print(f"âŒ ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
    sys.exit(1)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)

class LocalPipelineManager:
    """ë¡œì»¬ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € (ì¸ë„¤ì¼ ì œê±°)"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.crawler = None
        self.summarizer = None
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self._create_directories()
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self._initialize_components()
    
    def _create_directories(self):
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
        directories = [PDF_DIR, MD_DIR]  # ì¸ë„¤ì¼ ë””ë ‰í† ë¦¬ ì œê±°
        for directory in directories:
            Path(directory).mkdir(parents=True, exist_ok=True)
            logger.info(f"ğŸ“ ë””ë ‰í† ë¦¬ í™•ì¸: {directory}")
    
    def _initialize_components(self):
        """ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        try:
            # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
            self.crawler = BusanNewsCrawler()
            logger.info("âœ… í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
            
            # ìš”ì•½ê¸° ì´ˆê¸°í™” (API í‚¤ê°€ ìˆì„ ë•Œë§Œ)
            if OPENAI_API_KEY:
                self.summarizer = BusanNewsSummarizer()
                logger.info("âœ… ìš”ì•½ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                logger.warning("âš ï¸ OpenAI API í‚¤ê°€ ì—†ì–´ ìš”ì•½ê¸°ë¥¼ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤")
                
        except Exception as e:
            logger.error(f"âŒ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
    
    def run_crawling(self, max_pages: int = 5) -> List[Dict]:
        """ğŸ”§ ë³´ë„ìë£Œ í¬ë¡¤ë§ ì‹¤í–‰"""
        logger.info(f"ğŸ•·ï¸ ë¶€ì‚°ì‹œ ë³´ë„ìë£Œ í¬ë¡¤ë§ ì‹œì‘ (ìµœëŒ€ {max_pages}í˜ì´ì§€)...")
        
        try:
            if not self.crawler:
                logger.error("âŒ í¬ë¡¤ëŸ¬ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                return []
            
            # ğŸ”§ crawl_all â†’ crawl_newsë¡œ ë³€ê²½
            crawler_results = self.crawler.crawl_news(max_pages=max_pages)
            
            if crawler_results:
                logger.info(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {len(crawler_results)}ê°œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ")
                
                # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
                for result in crawler_results[:3]:  # ì²˜ìŒ 3ê°œë§Œ
                    logger.info(f"ğŸ“„ {result['filename']} â†’ {result['url']}")
                
                if len(crawler_results) > 3:
                    logger.info(f"... ì™¸ {len(crawler_results) - 3}ê°œ")
            else:
                logger.warning("âš ï¸ í¬ë¡¤ë§ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            return crawler_results
            
        except Exception as e:
            logger.error(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return []
    
    def run_summarization(self, crawler_results: List[Dict] = None) -> List[str]:
        """ğŸ”§ ë³´ë„ìë£Œ ìš”ì•½ ìƒì„± (URL ë§¤í•‘ ê°œì„ )"""
        logger.info("ğŸ“ ë³´ë„ìë£Œ ìš”ì•½ ìƒì„± ì‹œì‘...")
        
        if not self.summarizer:
            logger.warning("âš ï¸ ìš”ì•½ê¸°ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ì¡´ MD íŒŒì¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            existing_md_files = list(Path(MD_DIR).glob("*.md"))
            return [str(f) for f in existing_md_files]
        
        try:
            if crawler_results:
                # ğŸ”§ í¬ë¡¤ëŸ¬ ê²°ê³¼ë¥¼ ì§ì ‘ í™œìš©í•œ ìš”ì•½ ì²˜ë¦¬
                processed_files = []
                for result in crawler_results:
                    try:
                        pdf_path = Path(result['path'])  # 'filepath' â†’ 'path'ë¡œ ìˆ˜ì •
                        if pdf_path.exists():
                            md_file = self.summarizer.process_pdf_file(
                                str(pdf_path), 
                                source_url=result.get('url', 'https://www.busan.go.kr/nbtnewsBU')
                            )
                            if md_file:
                                processed_files.append(md_file)
                                logger.info(f"âœ… ìš”ì•½ ì™„ë£Œ: {Path(md_file).name}")
                            else:
                                logger.warning(f"âš ï¸ ìš”ì•½ ì‹¤íŒ¨: {pdf_path.name}")
                        else:
                            logger.warning(f"âš ï¸ PDF íŒŒì¼ ì—†ìŒ: {pdf_path}")
                    except Exception as e:
                        logger.error(f"âŒ PDF ì²˜ë¦¬ ì‹¤íŒ¨ {result.get('filename', 'unknown')}: {e}")
                        continue
            else:
                # PDF í´ë”ì—ì„œ íŒŒì¼ ìŠ¤ìº”
                pdf_files = list(Path(PDF_DIR).glob("*.pdf"))
                if not pdf_files:
                    logger.warning("âš ï¸ ì²˜ë¦¬í•  PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
                    return []
                
                logger.info(f"ğŸ“‹ ì²˜ë¦¬í•  PDF íŒŒì¼: {len(pdf_files)}ê°œ")
                processed_files = []
                
                for pdf_path in pdf_files:
                    try:
                        # ê¸°ë³¸ URLë¡œ ì²˜ë¦¬
                        md_file = self.summarizer.process_pdf_file(
                            str(pdf_path), 
                            source_url="https://www.busan.go.kr/nbtnewsBU"
                        )
                        
                        if md_file:
                            processed_files.append(md_file)
                            logger.info(f"âœ… ìš”ì•½ ì™„ë£Œ: {Path(md_file).name}")
                        else:
                            logger.warning(f"âš ï¸ ìš”ì•½ ì‹¤íŒ¨: {pdf_path.name}")
                    
                    except Exception as e:
                        logger.error(f"âŒ PDF ì²˜ë¦¬ ì‹¤íŒ¨ {pdf_path.name}: {e}")
                        continue
            
            logger.info(f"ğŸ‰ ìš”ì•½ ìƒì„± ì™„ë£Œ: {len(processed_files)}ê°œ")
            return processed_files
            
        except Exception as e:
            logger.error(f"âŒ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return []

def parse_arguments():
    """ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹±"""
    parser = argparse.ArgumentParser(description="ë¶€ì‚°ì‹œì²­ ë³´ë„ìë£Œ í¬í„¸ ë¡œì»¬ ì‹¤í–‰")
    
    # ì‹¤í–‰ ëª¨ë“œ
    group = parser.add_mutually_exclusive_group()
    group.add_argument('--crawl-only', action='store_true', help='í¬ë¡¤ë§ë§Œ ì‹¤í–‰')
    group.add_argument('--summarize-only', action='store_true', help='ìš”ì•½ë§Œ ì‹¤í–‰')
    
    # ì„¤ì •
    parser.add_argument('--max-pages', type=int, default=5, help='ìµœëŒ€ í¬ë¡¤ë§ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸: 5)')
    parser.add_argument('--test', action='store_true', help='í…ŒìŠ¤íŠ¸ ëª¨ë“œ (2í˜ì´ì§€ë§Œ)')
    
    return parser.parse_args()

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        logger.info("ğŸš€ ë¶€ì‚°ì‹œì²­ ë³´ë„ìë£Œ í¬í„¸ ë¡œì»¬ ì‹¤í–‰ ì‹œì‘")
        logger.info(f"ğŸ“ ì‹¤í–‰ í™˜ê²½: {'ë¡œì»¬' if IS_LOCAL else 'ë°°í¬'}")
        
        # ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹±
        args = parse_arguments()
        
        # íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        manager = LocalPipelineManager()
        
        # ì‹¤í–‰ ëª¨ë“œì— ë”°ë¥¸ ì²˜ë¦¬
        if args.crawl_only:
            # í¬ë¡¤ë§ë§Œ ì‹¤í–‰
            max_pages = 2 if args.test else args.max_pages
            manager.run_crawling(max_pages)
            
        elif args.summarize_only:
            # ìš”ì•½ë§Œ ì‹¤í–‰
            manager.run_summarization()
            
        else:
            # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ì¸ë„¤ì¼ ì œì™¸)
            max_pages = 2 if args.test else args.max_pages
            
            logger.info("ğŸ”„ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘ (í¬ë¡¤ë§ + ìš”ì•½)")
            
            # 1. í¬ë¡¤ë§
            logger.info("1ï¸âƒ£ í¬ë¡¤ë§ ë‹¨ê³„")
            crawler_results = manager.run_crawling(max_pages)
            
            # 2. ìš”ì•½ ìƒì„±
            logger.info("2ï¸âƒ£ ìš”ì•½ ìƒì„± ë‹¨ê³„")
            md_files = manager.run_summarization(crawler_results)
            
            # ê²°ê³¼ ìš”ì•½
            logger.info("ğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
            logger.info(f"ğŸ“Š ê²°ê³¼ ìš”ì•½:")
            logger.info(f"   - í¬ë¡¤ë§: {len(crawler_results)}ê°œ PDF")
            logger.info(f"   - ìš”ì•½: {len(md_files)}ê°œ MD")
        
        logger.info("âœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    except KeyboardInterrupt:
        logger.info("â¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
        sys.exit(0)
        
    except Exception as e:
        logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        sys.exit(1)

if __name__ == "__main__":
    main()