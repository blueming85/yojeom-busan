"""
ë¶€ì‚°ì‹œì²­ ë³´ë„ìë£Œ í¬í„¸ - ë¡œì»¬ í†µí•© ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
=================================================
í¬ë¡¤ë§ â†’ ìš”ì•½ê¹Œì§€ ì›í´ë¦­ ì‹¤í–‰ (ì‹ ê·œ PDFë§Œ ì²˜ë¦¬)

ì‚¬ìš©ë²•:
    python local_main.py                    # ì „ì²´ ì‹¤í–‰
    python local_main.py --crawl-only       # í¬ë¡¤ë§ë§Œ
    python local_main.py --summarize-only   # ìš”ì•½ë§Œ
    python local_main.py --test             # í…ŒìŠ¤íŠ¸ ëª¨ë“œ (2í˜ì´ì§€)
    python local_main.py --max-pages 5      # ìµœëŒ€ í˜ì´ì§€ ìˆ˜ ì§€ì •
"""

import os
import sys
import logging
import argparse
import traceback
from pathlib import Path
from typing import List, Dict, Optional

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent))

# ì»´í¬ë„ŒíŠ¸ import
try:
    from core.crawler import BusanNewsCrawler
    from core.summarizer import BusanNewsSummarizer
    from config import (
        LOG_FILE, PDF_DIR, MD_DIR,
        OPENAI_API_KEY, IS_LOCAL, IS_DEPLOYMENT
    )
except ImportError as e:
    print(f"âŒ ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
    sys.exit(1)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)

class LocalPipelineManager:
    """ë¡œì»¬ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € (ì‹ ê·œ PDFë§Œ ì²˜ë¦¬)"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.crawler = None
        self.summarizer = None
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self._create_directories()
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self._initialize_components()
    
    def _create_directories(self):
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
        directories = [PDF_DIR, MD_DIR]
        for directory in directories:
            Path(directory).mkdir(parents=True, exist_ok=True)
            logger.info(f"ğŸ“ ë””ë ‰í† ë¦¬ í™•ì¸: {directory}")
    
    def _initialize_components(self):
        """ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        try:
            # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
            self.crawler = BusanNewsCrawler()
            logger.info("âœ… í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
            
            # ìš”ì•½ê¸° ì´ˆê¸°í™” (API í‚¤ê°€ ìˆì„ ë•Œë§Œ)
            if OPENAI_API_KEY:
                self.summarizer = BusanNewsSummarizer()
                logger.info("âœ… ìš”ì•½ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                logger.warning("âš ï¸ OpenAI API í‚¤ê°€ ì—†ì–´ ìš”ì•½ê¸°ë¥¼ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤")
                
        except Exception as e:
            logger.error(f"âŒ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
    
    def run_crawling(self, max_pages: int = 5) -> List[Dict]:
        """ë³´ë„ìë£Œ í¬ë¡¤ë§ ì‹¤í–‰"""
        logger.info(f"ğŸ•·ï¸ ë¶€ì‚°ì‹œ ë³´ë„ìë£Œ í¬ë¡¤ë§ ì‹œì‘ (ìµœëŒ€ {max_pages}í˜ì´ì§€)...")
        
        try:
            if not self.crawler:
                logger.error("âŒ í¬ë¡¤ëŸ¬ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                return []
            
            # í¬ë¡¤ë§ ì‹¤í–‰
            crawler_results = self.crawler.crawl_news(max_pages=max_pages)
            
            if crawler_results:
                logger.info(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {len(crawler_results)}ê°œ íŒŒì¼ ì²˜ë¦¬")
                
                # ğŸ”§ ì‹ ê·œ/ê¸°ì¡´ íŒŒì¼ ë¶„ë¥˜
                new_files = [r for r in crawler_results if r.get('status') == 'downloaded']
                existing_files = [r for r in crawler_results if r.get('status') in ['already_exists', 'already_exists_similar']]
                
                logger.info(f"   ğŸ“¥ ì‹ ê·œ ë‹¤ìš´ë¡œë“œ: {len(new_files)}ê°œ")
                logger.info(f"   ğŸ“‹ ê¸°ì¡´ íŒŒì¼: {len(existing_files)}ê°œ")
                
                # ê²°ê³¼ ìš”ì•½ ì¶œë ¥ (ì‹ ê·œ íŒŒì¼ë§Œ)
                for result in new_files[:3]:
                    logger.info(f"ğŸ“„ ì‹ ê·œ: {result['filename']} â†’ {result['url']}")
                
                if len(new_files) > 3:
                    logger.info(f"... ì™¸ {len(new_files) - 3}ê°œ ì‹ ê·œ íŒŒì¼")
            else:
                logger.warning("âš ï¸ í¬ë¡¤ë§ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            return crawler_results
            
        except Exception as e:
            logger.error(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return []
    
    def run_summarization(self, crawler_results: List[Dict] = None) -> List[str]:
        """ğŸ”§ ì‹ ê·œ í¬ë¡¤ë§ëœ PDFë§Œ ìš”ì•½ ìƒì„±"""
        logger.info("ğŸ“ ë³´ë„ìë£Œ ìš”ì•½ ìƒì„± ì‹œì‘...")
        
        if not self.summarizer:
            logger.warning("âš ï¸ ìš”ì•½ê¸°ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ì¡´ MD íŒŒì¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            existing_md_files = list(Path(MD_DIR).glob("*.md"))
            return [str(f) for f in existing_md_files]
        
        if not crawler_results:
            logger.warning("âš ï¸ í¬ë¡¤ëŸ¬ ê²°ê³¼ê°€ ì—†ì–´ ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤")
            return []
        
        try:
            # ğŸ”§ ì‹ ê·œ ë‹¤ìš´ë¡œë“œëœ PDFë§Œ í•„í„°ë§
            new_pdfs = [
                result for result in crawler_results 
                if result.get('status') == 'downloaded'  # ìƒˆë¡œ ë‹¤ìš´ë¡œë“œëœ ê²ƒë§Œ
            ]
            
            if not new_pdfs:
                logger.info("ğŸ“‹ ì‹ ê·œ ë‹¤ìš´ë¡œë“œëœ PDFê°€ ì—†ì–´ ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤")
                
                # ê¸°ì¡´ MD íŒŒì¼ ëª©ë¡ ë°˜í™˜
                existing_md_files = list(Path(MD_DIR).glob("*.md"))
                logger.info(f"ğŸ“„ ê¸°ì¡´ MD íŒŒì¼: {len(existing_md_files)}ê°œ")
                return [str(f) for f in existing_md_files]
            
            logger.info(f"ğŸ“‹ ì‹ ê·œ PDF íŒŒì¼: {len(new_pdfs)}ê°œ ìš”ì•½ ìƒì„± ì‹œì‘")
            processed_files = []
            
            for idx, pdf_info in enumerate(new_pdfs, 1):
                try:
                    pdf_path = pdf_info['path']
                    source_url = pdf_info['url']
                    pdf_filename = pdf_info['filename']
                    
                    logger.info(f"[{idx}/{len(new_pdfs)}] ğŸ”— ì‹ ê·œ ì²˜ë¦¬: {pdf_filename}")
                    logger.info(f"   ğŸ“‚ íŒŒì¼: {pdf_path}")
                    logger.info(f"   ğŸŒ URL: {source_url}")
                    
                    # PDF ì²˜ë¦¬
                    md_file = self.summarizer.process_pdf_file(pdf_path, source_url)
                    
                    if md_file:
                        processed_files.append(md_file)
                        logger.info(f"âœ… ìš”ì•½ ì™„ë£Œ: {Path(md_file).name}")
                    else:
                        logger.warning(f"âš ï¸ ìš”ì•½ ì‹¤íŒ¨: {pdf_filename}")
                
                except Exception as e:
                    logger.error(f"âŒ PDF ì²˜ë¦¬ ì‹¤íŒ¨ {pdf_info.get('filename', 'unknown')}: {e}")
                    continue
            
            logger.info(f"ğŸ‰ ì‹ ê·œ ìš”ì•½ ìƒì„± ì™„ë£Œ: {len(processed_files)}ê°œ")
            
            # ì „ì²´ MD íŒŒì¼ ëª©ë¡ ë°˜í™˜ (ê¸°ì¡´ + ì‹ ê·œ)
            all_md_files = list(Path(MD_DIR).glob("*.md"))
            logger.info(f"ğŸ“Š ì „ì²´ MD íŒŒì¼: {len(all_md_files)}ê°œ")
            
            return [str(f) for f in all_md_files]
            
        except Exception as e:
            logger.error(f"âŒ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return []

def parse_arguments():
    """ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹±"""
    parser = argparse.ArgumentParser(description="ë¶€ì‚°ì‹œì²­ ë³´ë„ìë£Œ í¬í„¸ ë¡œì»¬ ì‹¤í–‰")
    
    # ì‹¤í–‰ ëª¨ë“œ
    group = parser.add_mutually_exclusive_group()
    group.add_argument('--crawl-only', action='store_true', help='í¬ë¡¤ë§ë§Œ ì‹¤í–‰')
    group.add_argument('--summarize-only', action='store_true', help='ìš”ì•½ë§Œ ì‹¤í–‰')
    
    # ì„¤ì •
    parser.add_argument('--max-pages', type=int, default=5, help='ìµœëŒ€ í¬ë¡¤ë§ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸: 5)')
    parser.add_argument('--test', action='store_true', help='í…ŒìŠ¤íŠ¸ ëª¨ë“œ (2í˜ì´ì§€ë§Œ)')
    
    return parser.parse_args()

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        logger.info("ğŸš€ ë¶€ì‚°ì‹œì²­ ë³´ë„ìë£Œ í¬í„¸ ë¡œì»¬ ì‹¤í–‰ ì‹œì‘")
        logger.info(f"ğŸ“ ì‹¤í–‰ í™˜ê²½: {'ë¡œì»¬' if IS_LOCAL else 'ë°°í¬'}")
        
        # ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹±
        args = parse_arguments()
        
        # íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        manager = LocalPipelineManager()
        
        # ì‹¤í–‰ ëª¨ë“œì— ë”°ë¥¸ ì²˜ë¦¬
        if args.crawl_only:
            # í¬ë¡¤ë§ë§Œ ì‹¤í–‰
            max_pages = 2 if args.test else args.max_pages
            crawler_results = manager.run_crawling(max_pages)
            
            logger.info("âœ… í¬ë¡¤ë§ ì‘ì—… ì™„ë£Œ!")
            
            # ğŸ”§ ì‹ ê·œ/ê¸°ì¡´ íŒŒì¼ í†µê³„
            new_count = len([r for r in crawler_results if r.get('status') == 'downloaded'])
            existing_count = len(crawler_results) - new_count
            
            logger.info(f"ğŸ“Š ê²°ê³¼: ì´ {len(crawler_results)}ê°œ (ì‹ ê·œ {new_count}ê°œ, ê¸°ì¡´ {existing_count}ê°œ)")
            
        elif args.summarize_only:
            # ìš”ì•½ë§Œ ì‹¤í–‰ (ê¸°ì¡´ PDF ëŒ€ìƒ)
            md_files = manager.run_summarization()
            
            logger.info("âœ… ìš”ì•½ ì‘ì—… ì™„ë£Œ!")
            logger.info(f"ğŸ“Š ê²°ê³¼: {len(md_files)}ê°œ MD íŒŒì¼")
            
        else:
            # ğŸ”§ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ì‹ ê·œ PDFë§Œ ìš”ì•½)
            max_pages = 2 if args.test else args.max_pages
            
            logger.info("ğŸ”„ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘ (í¬ë¡¤ë§ + ì‹ ê·œ PDF ìš”ì•½)")
            
            # 1. í¬ë¡¤ë§
            logger.info("1ï¸âƒ£ í¬ë¡¤ë§ ë‹¨ê³„")
            crawler_results = manager.run_crawling(max_pages)
            
            # 2. ì‹ ê·œ PDFë§Œ ìš”ì•½ ìƒì„±
            logger.info("2ï¸âƒ£ ì‹ ê·œ PDF ìš”ì•½ ìƒì„± ë‹¨ê³„")
            md_files = manager.run_summarization(crawler_results)
            
            # ê²°ê³¼ ìš”ì•½
            new_pdfs = len([r for r in crawler_results if r.get('status') == 'downloaded'])
            existing_pdfs = len(crawler_results) - new_pdfs
            
            logger.info("ğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
            logger.info(f"ğŸ“Š ìµœì¢… ê²°ê³¼:")
            logger.info(f"   - í¬ë¡¤ë§: {len(crawler_results)}ê°œ PDF (ì‹ ê·œ {new_pdfs}ê°œ, ê¸°ì¡´ {existing_pdfs}ê°œ)")
            logger.info(f"   - ìš”ì•½: {len(md_files)}ê°œ MD íŒŒì¼")
            logger.info(f"   - ì‹ ê·œ ìš”ì•½: {new_pdfs}ê°œ (ì‹ ê·œ PDFë§Œ ì²˜ë¦¬)")
        
        logger.info("âœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    except KeyboardInterrupt:
        logger.info("â¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
        sys.exit(0)
        
    except Exception as e:
        logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        sys.exit(1)

if __name__ == "__main__":
    main()