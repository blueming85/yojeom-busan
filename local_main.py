"""
ë¶€ì‚°ì‹œì²­ ë³´ë„ìë£Œ í¬í„¸ - ë¡œì»¬ í†µí•© ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
=================================================
í¬ë¡¤ë§ â†’ ìš”ì•½ê¹Œì§€ ì›í´ë¦­ ì‹¤í–‰ (URL ë§¤í•‘ ìˆ˜ì •)

ì‚¬ìš©ë²•:
    python local_main.py                    # ì „ì²´ ì‹¤í–‰
    python local_main.py --crawl-only       # í¬ë¡¤ë§ë§Œ
    python local_main.py --summarize-only   # ìš”ì•½ë§Œ
    python local_main.py --test             # í…ŒìŠ¤íŠ¸ ëª¨ë“œ (2í˜ì´ì§€)
    python local_main.py --max-pages 5      # ìµœëŒ€ í˜ì´ì§€ ìˆ˜ ì§€ì •
"""

import os
import sys
import logging
import argparse
import traceback
from pathlib import Path
from typing import List, Dict, Optional

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent))

# ì»´í¬ë„ŒíŠ¸ import
try:
    from core.crawler import BusanNewsCrawler
    from core.summarizer import BusanNewsSummarizer
    from config import (
        LOG_FILE, PDF_DIR, MD_DIR,
        OPENAI_API_KEY, IS_LOCAL, IS_DEPLOYMENT
    )
except ImportError as e:
    print(f"âŒ ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
    sys.exit(1)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)

class LocalPipelineManager:
    """ë¡œì»¬ íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € (URL ë§¤í•‘ ìˆ˜ì •)"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.crawler = None
        self.summarizer = None
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self._create_directories()
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self._initialize_components()
    
    def _create_directories(self):
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
        directories = [PDF_DIR, MD_DIR]
        for directory in directories:
            Path(directory).mkdir(parents=True, exist_ok=True)
            logger.info(f"ğŸ“ ë””ë ‰í† ë¦¬ í™•ì¸: {directory}")
    
    def _initialize_components(self):
        """ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        try:
            # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
            self.crawler = BusanNewsCrawler()
            logger.info("âœ… í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
            
            # ìš”ì•½ê¸° ì´ˆê¸°í™” (API í‚¤ê°€ ìˆì„ ë•Œë§Œ)
            if OPENAI_API_KEY:
                self.summarizer = BusanNewsSummarizer()
                logger.info("âœ… ìš”ì•½ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                logger.warning("âš ï¸ OpenAI API í‚¤ê°€ ì—†ì–´ ìš”ì•½ê¸°ë¥¼ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤")
                
        except Exception as e:
            logger.error(f"âŒ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
    
    def run_crawling(self, max_pages: int = 5) -> List[Dict]:
        """ë³´ë„ìë£Œ í¬ë¡¤ë§ ì‹¤í–‰"""
        logger.info(f"ğŸ•·ï¸ ë¶€ì‚°ì‹œ ë³´ë„ìë£Œ í¬ë¡¤ë§ ì‹œì‘ (ìµœëŒ€ {max_pages}í˜ì´ì§€)...")
        
        try:
            if not self.crawler:
                logger.error("âŒ í¬ë¡¤ëŸ¬ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                return []
            
            # í¬ë¡¤ë§ ì‹¤í–‰
            crawler_results = self.crawler.crawl_news(max_pages=max_pages)
            
            if crawler_results:
                logger.info(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {len(crawler_results)}ê°œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ")
                
                # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
                for result in crawler_results[:3]:  # ì²˜ìŒ 3ê°œë§Œ
                    logger.info(f"ğŸ“„ {result['filename']} â†’ {result['url']}")
                
                if len(crawler_results) > 3:
                    logger.info(f"... ì™¸ {len(crawler_results) - 3}ê°œ")
            else:
                logger.warning("âš ï¸ í¬ë¡¤ë§ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            return crawler_results
            
        except Exception as e:
            logger.error(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return []
    
    def run_summarization(self, crawler_results: List[Dict] = None) -> List[str]:
        """ğŸ”§ ë³´ë„ìë£Œ ìš”ì•½ ìƒì„± (URL ë§¤í•‘ ìˆ˜ì •)"""
        logger.info("ğŸ“ ë³´ë„ìë£Œ ìš”ì•½ ìƒì„± ì‹œì‘...")
        
        if not self.summarizer:
            logger.warning("âš ï¸ ìš”ì•½ê¸°ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ì¡´ MD íŒŒì¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            existing_md_files = list(Path(MD_DIR).glob("*.md"))
            return [str(f) for f in existing_md_files]
        
        try:
            # ğŸ”§ URL ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ìƒì„±
            url_mapping = {}
            if crawler_results:
                logger.info(f"ğŸ”— URL ë§¤í•‘ ìƒì„±: {len(crawler_results)}ê°œ")
                for result in crawler_results:
                    # íŒŒì¼ëª…ì„ í‚¤ë¡œ URLì„ ê°’ìœ¼ë¡œ ë§¤í•‘
                    filename = result.get('filename', '')
                    url = result.get('url', '')
                    
                    if filename and url:
                        url_mapping[filename] = url
                        logger.debug(f"   ğŸ“ {filename} â†’ {url}")
                
                logger.info(f"âœ… URL ë§¤í•‘ ì™„ë£Œ: {len(url_mapping)}ê°œ")
            else:
                logger.info("ğŸ”— í¬ë¡¤ëŸ¬ ê²°ê³¼ê°€ ì—†ì–´ URL ë§¤í•‘ì„ ê±´ë„ˆëœë‹ˆë‹¤")
            
            # ì²˜ë¦¬í•  PDF íŒŒì¼ë“¤ í™•ì¸
            pdf_files = list(Path(PDF_DIR).glob("*.pdf"))
            if not pdf_files:
                logger.warning("âš ï¸ ì²˜ë¦¬í•  PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
                return []
            
            logger.info(f"ğŸ“‹ ì²˜ë¦¬í•  PDF íŒŒì¼: {len(pdf_files)}ê°œ")
            processed_files = []
            
            for pdf_path in pdf_files:
                try:
                    pdf_filename = pdf_path.name
                    
                    # ğŸ”§ URL ë§¤í•‘ì—ì„œ í•´ë‹¹ íŒŒì¼ì˜ URL ì°¾ê¸°
                    source_url = url_mapping.get(pdf_filename, "")
                    
                    if source_url:
                        logger.info(f"ğŸ”— URL ë§¤í•‘ ì ìš©: {pdf_filename} â†’ {source_url}")
                    else:
                        logger.warning(f"âš ï¸ URL ë§¤í•‘ ì—†ìŒ: {pdf_filename} (ê¸°ë³¸ URL ì‚¬ìš©)")
                        source_url = "https://www.busan.go.kr/nbtnewsBU"
                    
                    # PDF ì²˜ë¦¬
                    md_file = self.summarizer.process_pdf_file(
                        str(pdf_path), 
                        source_url=source_url
                    )
                    
                    if md_file:
                        processed_files.append(md_file)
                        logger.info(f"âœ… ìš”ì•½ ì™„ë£Œ: {Path(md_file).name}")
                    else:
                        logger.warning(f"âš ï¸ ìš”ì•½ ì‹¤íŒ¨: {pdf_filename}")
                
                except Exception as e:
                    logger.error(f"âŒ PDF ì²˜ë¦¬ ì‹¤íŒ¨ {pdf_path.name}: {e}")
                    continue
            
            logger.info(f"ğŸ‰ ìš”ì•½ ìƒì„± ì™„ë£Œ: {len(processed_files)}ê°œ")
            
            # ğŸ”§ URL ë§¤í•‘ í†µê³„ ì¶œë ¥
            if url_mapping:
                mapped_count = sum(1 for f in processed_files if any(
                    Path(f).stem.endswith(Path(pdf).stem) for pdf in pdf_files 
                    if pdf.name in url_mapping
                ))
                logger.info(f"ğŸ“Š URL ë§¤í•‘ ì ìš©ë¥ : {mapped_count}/{len(processed_files)} ({mapped_count/len(processed_files)*100:.1f}%)")
            
            return processed_files
            
        except Exception as e:
            logger.error(f"âŒ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return []

def parse_arguments():
    """ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹±"""
    parser = argparse.ArgumentParser(description="ë¶€ì‚°ì‹œì²­ ë³´ë„ìë£Œ í¬í„¸ ë¡œì»¬ ì‹¤í–‰")
    
    # ì‹¤í–‰ ëª¨ë“œ
    group = parser.add_mutually_exclusive_group()
    group.add_argument('--crawl-only', action='store_true', help='í¬ë¡¤ë§ë§Œ ì‹¤í–‰')
    group.add_argument('--summarize-only', action='store_true', help='ìš”ì•½ë§Œ ì‹¤í–‰')
    
    # ì„¤ì •
    parser.add_argument('--max-pages', type=int, default=5, help='ìµœëŒ€ í¬ë¡¤ë§ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸: 5)')
    parser.add_argument('--test', action='store_true', help='í…ŒìŠ¤íŠ¸ ëª¨ë“œ (2í˜ì´ì§€ë§Œ)')
    
    return parser.parse_args()

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        logger.info("ğŸš€ ë¶€ì‚°ì‹œì²­ ë³´ë„ìë£Œ í¬í„¸ ë¡œì»¬ ì‹¤í–‰ ì‹œì‘")
        logger.info(f"ğŸ“ ì‹¤í–‰ í™˜ê²½: {'ë¡œì»¬' if IS_LOCAL else 'ë°°í¬'}")
        
        # ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹±
        args = parse_arguments()
        
        # íŒŒì´í”„ë¼ì¸ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        manager = LocalPipelineManager()
        
        # ì‹¤í–‰ ëª¨ë“œì— ë”°ë¥¸ ì²˜ë¦¬
        if args.crawl_only:
            # í¬ë¡¤ë§ë§Œ ì‹¤í–‰
            max_pages = 2 if args.test else args.max_pages
            crawler_results = manager.run_crawling(max_pages)
            
            logger.info("âœ… í¬ë¡¤ë§ ì‘ì—… ì™„ë£Œ!")
            logger.info(f"ğŸ“Š ê²°ê³¼: {len(crawler_results)}ê°œ PDF ë‹¤ìš´ë¡œë“œ")
            
        elif args.summarize_only:
            # ìš”ì•½ë§Œ ì‹¤í–‰
            md_files = manager.run_summarization()
            
            logger.info("âœ… ìš”ì•½ ì‘ì—… ì™„ë£Œ!")
            logger.info(f"ğŸ“Š ê²°ê³¼: {len(md_files)}ê°œ MD ìƒì„±")
            
        else:
            # ğŸ”§ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (URL ë§¤í•‘ í¬í•¨)
            max_pages = 2 if args.test else args.max_pages
            
            logger.info("ğŸ”„ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘ (í¬ë¡¤ë§ + ìš”ì•½ + URL ë§¤í•‘)")
            
            # 1. í¬ë¡¤ë§
            logger.info("1ï¸âƒ£ í¬ë¡¤ë§ ë‹¨ê³„")
            crawler_results = manager.run_crawling(max_pages)
            
            # 2. ìš”ì•½ ìƒì„± (URL ë§¤í•‘ í¬í•¨)
            logger.info("2ï¸âƒ£ ìš”ì•½ ìƒì„± ë‹¨ê³„ (URL ë§¤í•‘ ì ìš©)")
            md_files = manager.run_summarization(crawler_results)
            
            # ê²°ê³¼ ìš”ì•½
            logger.info("ğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
            logger.info(f"ğŸ“Š ìµœì¢… ê²°ê³¼:")
            logger.info(f"   - í¬ë¡¤ë§: {len(crawler_results)}ê°œ PDF")
            logger.info(f"   - ìš”ì•½: {len(md_files)}ê°œ MD")
            
            # ğŸ”§ URL ë§¤í•‘ ì„±ê³µ ì—¬ë¶€ í™•ì¸
            if crawler_results and md_files:
                logger.info(f"   - URL ë§¤í•‘: âœ… ì„±ê³µ (í¬ë¡¤ëŸ¬ ê²°ê³¼ í™œìš©)")
            else:
                logger.info(f"   - URL ë§¤í•‘: âš ï¸ ë¶€ë¶„ì  (ê¸°ë³¸ URL ì‚¬ìš©)")
        
        logger.info("âœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    except KeyboardInterrupt:
        logger.info("â¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
        sys.exit(0)
        
    except Exception as e:
        logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        sys.exit(1)

if __name__ == "__main__":
    main()