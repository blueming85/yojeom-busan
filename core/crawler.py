import os
import time
import requests
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup, Tag
from urllib.parse import urljoin
from datetime import datetime
from typing import Optional
import logging
import re
from difflib import SequenceMatcher

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class BusanNewsCrawler:
    def __init__(self, download_dir="./data/pdfs"):
        self.base_list_url = "https://www.busan.go.kr/nbtnewsBU?curPage={}"
        self.base_url = "https://www.busan.go.kr"
        self.download_dir = download_dir
        self.driver = None
        
        # ë‹¤ìš´ë¡œë“œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.download_dir, exist_ok=True)
        
    def setup_driver(self):
        """Selenium ë“œë¼ì´ë²„ ì„¤ì •"""
        logger.info("Selenium ë“œë¼ì´ë²„ ì¤€ë¹„ ì¤‘...")
        options = webdriver.ChromeOptions()
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920,1080')
        
        try:
            self.driver = webdriver.Chrome(
                service=Service(ChromeDriverManager().install()), 
                options=options
            )
            logger.info("ë“œë¼ì´ë²„ ì¤€ë¹„ ì™„ë£Œ")
            return True
        except Exception as e:
            logger.error(f"ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨: {e}")
            return False
    
    def get_news_links(self, max_pages=5):
        """ğŸ”§ ë³´ë„ìë£Œ ìƒì„¸ í˜ì´ì§€ ë§í¬ë“¤ì„ ìˆ˜ì§‘ (ì œëª© ì¶”ì¶œ ê°œì„ )"""
        if not self.driver:
            logger.error("ë“œë¼ì´ë²„ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        all_links = []
        
        for page in range(1, max_pages + 1):
            logger.info(f'í˜ì´ì§€ {page} ìš”ì²­ ì¤‘...')
            try:
                self.driver.get(self.base_list_url.format(page))
                time.sleep(2)
                
                soup = BeautifulSoup(self.driver.page_source, "html.parser")
                news_links = soup.select("a.item")
                
                page_links = []
                for a_tag in news_links:
                    if isinstance(a_tag, Tag):
                        detail_href = a_tag.get("href")
                        if detail_href:
                            detail_url = urljoin(self.base_url, str(detail_href))
                            
                            # ğŸ”§ ì œëª© ì¶”ì¶œ ê°œì„  - ì—¬ëŸ¬ ì„ íƒì ì‹œë„
                            title = self._extract_title_from_list_item(a_tag)
                            
                            # ë‚ ì§œ ì •ë³´ ì¶”ì¶œ
                            date = self._extract_date_from_list_page(a_tag)
                            
                            page_links.append({
                                'url': detail_url,
                                'title': title,
                                'date': date,
                                'page': page
                            })
                
                logger.info(f'  - í˜ì´ì§€ {page}ì—ì„œ {len(page_links)}ê°œ ë§í¬ ë°œê²¬')
                all_links.extend(page_links)
                
            except Exception as e:
                logger.error(f"í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        logger.info(f"ì´ {len(all_links)}ê°œì˜ ë³´ë„ìë£Œ ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ")
        return all_links
    
    def _extract_title_from_list_item(self, a_tag: Tag) -> str:
        """ğŸ”§ ëª©ë¡ì—ì„œ ì œëª© ì¶”ì¶œ (ì—¬ëŸ¬ ì„ íƒì ì‹œë„)"""
        try:
            # 1ìˆœìœ„: .subject í´ë˜ìŠ¤
            title_elem = a_tag.select_one(".subject")
            if title_elem and title_elem.get_text(strip=True):
                title = title_elem.get_text(strip=True)
                logger.debug(f"ì œëª© ì¶”ì¶œ ì„±ê³µ (.subject): {title[:50]}...")
                return title
            
            # 2ìˆœìœ„: .title í´ë˜ìŠ¤
            title_elem = a_tag.select_one(".title")
            if title_elem and title_elem.get_text(strip=True):
                title = title_elem.get_text(strip=True)
                logger.debug(f"ì œëª© ì¶”ì¶œ ì„±ê³µ (.title): {title[:50]}...")
                return title
            
            # 3ìˆœìœ„: h3, h4 íƒœê·¸
            for tag in ['h3', 'h4', 'h2']:
                title_elem = a_tag.select_one(tag)
                if title_elem and title_elem.get_text(strip=True):
                    title = title_elem.get_text(strip=True)
                    logger.debug(f"ì œëª© ì¶”ì¶œ ì„±ê³µ ({tag}): {title[:50]}...")
                    return title
            
            # 4ìˆœìœ„: strong íƒœê·¸
            title_elem = a_tag.select_one("strong")
            if title_elem and title_elem.get_text(strip=True):
                title = title_elem.get_text(strip=True)
                logger.debug(f"ì œëª© ì¶”ì¶œ ì„±ê³µ (strong): {title[:50]}...")
                return title
            
            # 5ìˆœìœ„: a íƒœê·¸ ì§ì ‘ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
            all_text = a_tag.get_text(strip=True)
            if all_text and len(all_text) > 10:
                # ì²« ë²ˆì§¸ ì¤„ë§Œ ì œëª©ìœ¼ë¡œ ì‚¬ìš©
                first_line = all_text.split('\n')[0].strip()
                if len(first_line) > 10:
                    title = first_line[:100]  # ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
                    logger.debug(f"ì œëª© ì¶”ì¶œ ì„±ê³µ (ì „ì²´ í…ìŠ¤íŠ¸): {title[:50]}...")
                    return title
            
            logger.warning("ì œëª© ì¶”ì¶œ ì‹¤íŒ¨ - ëª¨ë“  ì„ íƒìì—ì„œ ë¹ˆ ê°’")
            return "ì œëª© ì—†ìŒ"
            
        except Exception as e:
            logger.error(f"ì œëª© ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return "ì œëª© ì—†ìŒ"
    
    def download_pdfs_from_page(self, news_item):
        """ğŸ”§ íŠ¹ì • ë³´ë„ìë£Œ í˜ì´ì§€ì—ì„œ PDF ë‹¤ìš´ë¡œë“œ (URL ë§¤í•‘ ê°œì„ )"""
        url = news_item['url']
        list_title = news_item['title']  # ëª©ë¡ì—ì„œ ê°€ì ¸ì˜¨ ì œëª©
        original_date = news_item.get('date')
        
        try:
            logger.info(f'ìƒì„¸í˜ì´ì§€ ì§„ì…: {list_title[:50]}...')
            self.driver.get(url)
            time.sleep(1.5)
            
            detail_soup = BeautifulSoup(self.driver.page_source, "html.parser")
            
            # ğŸ”§ ìƒì„¸ í˜ì´ì§€ì—ì„œ ë” ì •í™•í•œ ì œëª© ì¶”ì¶œ
            detail_title = self._extract_title_from_detail_page(detail_soup)
            final_title = detail_title if detail_title and detail_title != "ì œëª© ì—†ìŒ" else list_title
            
            # ìƒì„¸ í˜ì´ì§€ì—ì„œë„ ë‚ ì§œ ì¶”ì¶œ ì‹œë„
            detail_date = self._extract_date_from_detail_page(detail_soup)
            final_date = detail_date if detail_date else original_date
            
            # PDF ì²¨ë¶€íŒŒì¼ ë§í¬ ì°¾ê¸°
            pdf_links = detail_soup.find_all("a", string=lambda t: bool(t and ".pdf" in t))
            
            if not pdf_links:
                logger.info(f'      - PDF ë§í¬ ì—†ìŒ: {final_title[:50]}...')
                return []
            
            # ê¸°ì¡´ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            existing_files = [f for f in os.listdir(self.download_dir) if f.endswith('.pdf')]
            
            downloaded_files = []
            
            for pdf_link in pdf_links:
                if not isinstance(pdf_link, Tag):
                    continue
                    
                pdf_href = pdf_link.get('href')
                if not pdf_href:
                    continue
                
                pdf_url = urljoin(self.base_url, str(pdf_href))
                pdf_name = pdf_link.text.strip()
                
                # íŒŒì¼ëª… ì •ë¦¬
                safe_filename = self._clean_filename(pdf_name)
                if not safe_filename.endswith('.pdf'):
                    safe_filename += '.pdf'
                
                pdf_path = os.path.join(self.download_dir, safe_filename)
                
                # ì¤‘ë³µ ì²´í¬
                if os.path.exists(pdf_path):
                    logger.info(f"      - ì´ë¯¸ ì¡´ì¬ (ì •í™• ë§¤ì¹­): {safe_filename}")
                    downloaded_files.append({
                        'filename': safe_filename,
                        'path': pdf_path,
                        'title': final_title,
                        'date': final_date,
                        'url': url,  # ğŸ”§ ìƒì„¸ í˜ì´ì§€ URL
                        'status': 'already_exists'
                    })
                    continue
                
                # ìœ ì‚¬í•œ íŒŒì¼ëª… ì²´í¬
                similar_file = self._is_similar_pdf(safe_filename, existing_files)
                if similar_file:
                    similar_path = os.path.join(self.download_dir, similar_file)
                    logger.info(f"      - ì´ë¯¸ ì¡´ì¬ (ìœ ì‚¬ ë§¤ì¹­): {safe_filename} â‰ˆ {similar_file}")
                    downloaded_files.append({
                        'filename': similar_file,
                        'path': similar_path,
                        'title': final_title,
                        'date': final_date,
                        'url': url,  # ğŸ”§ ìƒì„¸ í˜ì´ì§€ URL
                        'status': 'already_exists_similar'
                    })
                    continue
                
                # ìƒˆ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                try:
                    logger.info(f"      ğŸ“¥ ë‹¤ìš´ë¡œë“œ: {safe_filename}")
                    pdf_response = requests.get(pdf_url, timeout=30)
                    pdf_response.raise_for_status()
                    
                    with open(pdf_path, "wb") as f:
                        f.write(pdf_response.content)
                    
                    existing_files.append(safe_filename)
                    
                    downloaded_files.append({
                        'filename': safe_filename,  # ğŸ”§ ì‹¤ì œ ì €ì¥ëœ íŒŒì¼ëª…
                        'path': pdf_path,
                        'title': final_title,
                        'date': final_date,
                        'url': url,  # ğŸ”§ ìƒì„¸ í˜ì´ì§€ URL (ì¤‘ìš”!)
                        'status': 'downloaded',
                        'size': len(pdf_response.content)
                    })
                    
                    logger.info(f"      âœ… ì™„ë£Œ: {safe_filename} ({len(pdf_response.content)/1024:.1f}KB)")
                    time.sleep(0.5)
                    
                except Exception as e:
                    logger.error(f"      âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {safe_filename} - {e}")
                    continue
            
            return downloaded_files
            
        except Exception as e:
            logger.error(f"í˜ì´ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {final_title[:50]}... - {e}")
            return []
    
    def _extract_title_from_detail_page(self, detail_soup) -> str:
        """ğŸ”§ ìƒì„¸ í˜ì´ì§€ì—ì„œ ì œëª© ì¶”ì¶œ"""
        try:
            # ìƒì„¸ í˜ì´ì§€ì˜ ì œëª© ì„ íƒìë“¤
            title_selectors = [
                ".view_title",
                ".board_view h1",
                ".news_title",
                ".article_title h1",
                "h1.title",
                ".content_title",
                "h1",
                ".subject"
            ]
            
            for selector in title_selectors:
                title_elem = detail_soup.select_one(selector)
                if title_elem and title_elem.get_text(strip=True):
                    title = title_elem.get_text(strip=True)
                    if len(title) > 5:  # ë„ˆë¬´ ì§§ì€ ì œëª© ì œì™¸
                        logger.debug(f"ìƒì„¸ í˜ì´ì§€ ì œëª© ì¶”ì¶œ: {title[:50]}...")
                        return title
            
            return "ì œëª© ì—†ìŒ"
            
        except Exception as e:
            logger.warning(f"ìƒì„¸ í˜ì´ì§€ ì œëª© ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return "ì œëª© ì—†ìŒ"
    
    def _is_similar_pdf(self, new_filename: str, existing_files: list) -> Optional[str]:
        """PDF íŒŒì¼ ì¤‘ë³µ ì²´í¬ (ê°„ì†Œí™”)"""
        try:
            if not existing_files:
                return None
            
            new_normalized = self._normalize_filename(new_filename)
            
            for existing_file in existing_files:
                if not existing_file.endswith('.pdf'):
                    continue
                
                existing_normalized = self._normalize_filename(existing_file)
                similarity = SequenceMatcher(None, new_normalized, existing_normalized).ratio()
                
                if similarity >= 0.85:
                    return existing_file
            
            return None
            
        except Exception as e:
            logger.error(f"âŒ ìœ ì‚¬ íŒŒì¼ ì²´í¬ ì‹¤íŒ¨: {e}")
            return None
    
    def _normalize_filename(self, filename: str) -> str:
        """íŒŒì¼ëª… ì •ê·œí™”"""
        try:
            normalized = filename.lower()
            normalized = re.sub(r'[ãˆœ().,\s_\-\[\]{}ã€ã€‘ã€Œã€ã€ã€""''ãƒ»]', '', normalized)
            normalized = re.sub(r'[^\wê°€-í£]', '', normalized)
            normalized = normalized.replace('.pdf', '')
            return normalized
        except Exception as e:
            logger.debug(f"íŒŒì¼ëª… ì •ê·œí™” ì‹¤íŒ¨: {filename} - {e}")
            return filename.lower()
    
    def _extract_date_from_list_page(self, news_item_tag):
        """ë³´ë„ìë£Œ ëª©ë¡ í˜ì´ì§€ì—ì„œ ë‚ ì§œ ì¶”ì¶œ"""
        try:
            date_selectors = [".date", ".reg_date", ".write_date", ".board_date"]
            
            for selector in date_selectors:
                date_elem = news_item_tag.select_one(selector)
                if date_elem:
                    date_text = date_elem.get_text(strip=True)
                    parsed_date = self._parse_date_string(date_text)
                    if parsed_date:
                        return parsed_date
            
            # ì„ íƒìë¡œ ì°¾ì§€ ëª»í•œ ê²½ìš°, í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ íŒ¨í„´ ì°¾ê¸°
            all_text = news_item_tag.get_text()
            parsed_date = self._parse_date_string(all_text)
            if parsed_date:
                return parsed_date
            
            return datetime.now().strftime("%Y-%m-%d")
            
        except Exception as e:
            logger.warning(f"ë‚ ì§œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return datetime.now().strftime("%Y-%m-%d")
    
    def _parse_date_string(self, date_text: str) -> Optional[str]:
        """ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ì„ YYYY-MM-DDë¡œ ë³€í™˜"""
        if not date_text:
            return None
        
        import re
        from datetime import datetime
        
        date_patterns = [
            (r'(\d{4})-(\d{1,2})-(\d{1,2})', r'\1-\2-\3'),
            (r'(\d{4})\.(\d{1,2})\.(\d{1,2})', r'\1-\2-\3'),
            (r'(\d{4})/(\d{1,2})/(\d{1,2})', r'\1-\2-\3'),
            (r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼', r'\1-\2-\3'),
            (r'(\d{1,2})-(\d{1,2})(?!\d)', f'{datetime.now().year}-\\1-\\2'),
            (r'(\d{1,2})\.(\d{1,2})(?!\d)', f'{datetime.now().year}-\\1-\\2'),
        ]
        
        for pattern, replacement in date_patterns:
            match = re.search(pattern, date_text)
            if match:
                try:
                    if len(pattern.split('(')) > 3:
                        year = int(match.group(1))
                        month = int(match.group(2))
                        day = int(match.group(3))
                    else:
                        year = datetime.now().year
                        month = int(match.group(1))
                        day = int(match.group(2))
                    
                    if 1 <= month <= 12 and 1 <= day <= 31:
                        date_obj = datetime(year, month, day)
                        return date_obj.strftime("%Y-%m-%d")
                except ValueError:
                    continue
        
        return None
    
    def _extract_date_from_detail_page(self, detail_soup) -> Optional[str]:
        """ìƒì„¸ í˜ì´ì§€ì—ì„œ ë‚ ì§œ ì •ë³´ ì¶”ì¶œ"""
        try:
            date_selectors = [
                ".view_info .date",
                ".board_view .date", 
                ".news_info .date",
                ".article_info .date",
                "td:contains('ì‘ì„±ì¼')",
                "td:contains('ë“±ë¡ì¼')",
                ".reg_date",
                ".write_date"
            ]
            
            for selector in date_selectors:
                date_elem = detail_soup.select_one(selector)
                if date_elem:
                    date_text = date_elem.get_text(strip=True)
                    parsed_date = self._parse_date_string(date_text)
                    if parsed_date:
                        return parsed_date
            
            page_text = detail_soup.get_text()
            parsed_date = self._parse_date_string(page_text)
            if parsed_date:
                return parsed_date
            
            return None
            
        except Exception as e:
            logger.warning(f"ìƒì„¸ í˜ì´ì§€ ë‚ ì§œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def _clean_filename(self, filename: str) -> str:
        """íŒŒì¼ëª…ì—ì„œ íŠ¹ìˆ˜ë¬¸ì ì œê±°"""
        import re
        cleaned = re.sub(r'[<>:"/\\|?*]', '_', filename)
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()
        return cleaned
    
    def crawl_news(self, max_pages=5):
        """ğŸ”§ ì „ì²´ í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ (URL ë§¤í•‘ ê°œì„ )"""
        logger.info("ë¶€ì‚°ì‹œì²­ ë³´ë„ìë£Œ í¬ë¡¤ë§ ì‹œì‘!")
        
        if not self.setup_driver():
            logger.error("í¬ë¡¤ë§ì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        try:
            # 1. ë³´ë„ìë£Œ ë§í¬ ìˆ˜ì§‘
            news_links = self.get_news_links(max_pages)
            if not news_links:
                logger.warning("ìˆ˜ì§‘ëœ ë³´ë„ìë£Œ ë§í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            # 2. ê° í˜ì´ì§€ì—ì„œ PDF ë‹¤ìš´ë¡œë“œ
            all_downloaded = []
            total_links = len(news_links)
            
            for idx, news_item in enumerate(news_links, 1):
                logger.info(f"[{idx}/{total_links}] ì²˜ë¦¬ ì¤‘: {news_item['title'][:50]}...")
                downloaded_files = self.download_pdfs_from_page(news_item)
                all_downloaded.extend(downloaded_files)
            
            # 3. ğŸ”§ URL ë§¤í•‘ ê²€ì¦ ë¡œê·¸
            self._print_url_mapping_summary(all_downloaded)
            
            return all_downloaded
            
        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []
        finally:
            self.close()
    
    def _print_url_mapping_summary(self, downloaded_files):
        """ğŸ”§ URL ë§¤í•‘ ìš”ì•½ ì¶œë ¥"""
        total = len(downloaded_files)
        new_downloads = len([f for f in downloaded_files if f.get('status') == 'downloaded'])
        existing = total - new_downloads
        
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½")
        logger.info("="*60)
        logger.info(f"ì´ ì²˜ë¦¬ëœ íŒŒì¼: {total}ê°œ")
        logger.info(f"ìƒˆë¡œ ë‹¤ìš´ë¡œë“œ: {new_downloads}ê°œ")
        logger.info(f"ê¸°ì¡´ íŒŒì¼: {existing}ê°œ")
        logger.info(f"ì €ì¥ ìœ„ì¹˜: {self.download_dir}")
        
        # ğŸ”§ URL ë§¤í•‘ ìƒ˜í”Œ ì¶œë ¥
        logger.info("\nğŸ”— URL ë§¤í•‘ ìƒ˜í”Œ:")
        for i, f in enumerate(downloaded_files[:3]):
            logger.info(f"  {i+1}. íŒŒì¼: {f['filename']}")
            logger.info(f"     URL: {f['url']}")
            logger.info(f"     ì œëª©: {f['title'][:50]}...")
        
        if len(downloaded_files) > 3:
            logger.info(f"  ... ì™¸ {len(downloaded_files) - 3}ê°œ")
        
        logger.info("="*60)
    
    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.driver:
            self.driver.quit()
            logger.info("ë“œë¼ì´ë²„ ì¢…ë£Œ ì™„ë£Œ")

def main():
    """í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì˜ˆì‹œ"""
    crawler = BusanNewsCrawler()
    downloaded_files = crawler.crawl_news(max_pages=2)
    
    print(f"\nğŸ¯ í¬ë¡¤ë§ ê²°ê³¼: {len(downloaded_files)}ê°œ íŒŒì¼")

if __name__ == "__main__":
    main()