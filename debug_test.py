#!/usr/bin/env python3
"""
URL ë§¤í•‘ ë””ë²„ê¹… í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
í˜„ì¬ í¬ë¡¤ëŸ¬ì™€ ìš”ì•½ê¸°ì˜ ì—°ë™ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
"""

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent))

from core.crawler import BusanNewsCrawler
from core.summarizer import BusanNewsSummarizer
import json

def test_crawler_output():
    """í¬ë¡¤ëŸ¬ ì¶œë ¥ í…ŒìŠ¤íŠ¸"""
    print("ğŸ” í¬ë¡¤ëŸ¬ ì¶œë ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    crawler = BusanNewsCrawler()
    
    # 1í˜ì´ì§€ë§Œ í…ŒìŠ¤íŠ¸
    results = crawler.crawl_news(max_pages=1)
    
    print(f"\nğŸ“Š í¬ë¡¤ë§ ê²°ê³¼: {len(results)}ê°œ")
    
    if results:
        print("\nğŸ“‹ ì²« ë²ˆì§¸ ê²°ê³¼:")
        first_result = results[0]
        print(json.dumps(first_result, indent=2, ensure_ascii=False))
        
        print(f"\nğŸ”‘ í¬í•¨ëœ í‚¤ë“¤: {list(first_result.keys())}")
        
        # URL í•„ë“œ í™•ì¸
        if 'url' in first_result:
            print(f"âœ… URL í•„ë“œ ì¡´ì¬: {first_result['url']}")
        else:
            print("âŒ URL í•„ë“œ ì—†ìŒ!")
            
        # ê²½ë¡œ í•„ë“œ í™•ì¸
        path_keys = [k for k in first_result.keys() if 'path' in k.lower()]
        print(f"ğŸ“ ê²½ë¡œ ê´€ë ¨ í‚¤ë“¤: {path_keys}")
    
    return results

def test_summarizer_input(crawler_results):
    """ìš”ì•½ê¸° ì…ë ¥ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ” ìš”ì•½ê¸° ì…ë ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    if not crawler_results:
        print("âŒ í¬ë¡¤ëŸ¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    summarizer = BusanNewsSummarizer()
    
    # ì²« ë²ˆì§¸ íŒŒì¼ë§Œ í…ŒìŠ¤íŠ¸
    test_result = crawler_results[0]
    
    print(f"ğŸ“„ í…ŒìŠ¤íŠ¸ íŒŒì¼: {test_result.get('filename', 'unknown')}")
    print(f"ğŸ”— URL: {test_result.get('url', 'N/A')}")
    
    # íŒŒì¼ ê²½ë¡œ í™•ì¸
    pdf_path = None
    for path_key in ['path', 'filepath', 'file_path', 'pdf_path']:
        if path_key in test_result:
            pdf_path = Path(test_result[path_key])
            print(f"ğŸ“ PDF ê²½ë¡œ ({path_key}): {pdf_path}")
            break
    
    if not pdf_path:
        filename = test_result.get('filename')
        if filename:
            pdf_path = Path("data/pdfs") / filename
            print(f"ğŸ“ íŒŒì¼ëª…ìœ¼ë¡œ ê²½ë¡œ êµ¬ì„±: {pdf_path}")
    
    if pdf_path and pdf_path.exists():
        print(f"âœ… PDF íŒŒì¼ ì¡´ì¬: {pdf_path}")
        
        # ìš”ì•½ í…ŒìŠ¤íŠ¸
        source_url = test_result.get('url', 'https://www.busan.go.kr/nbtnewsBU')
        print(f"ğŸ”— ì‚¬ìš©í•  URL: {source_url}")
        
        md_file = summarizer.process_pdf_file(str(pdf_path), source_url=source_url)
        
        if md_file:
            print(f"âœ… ìš”ì•½ ì„±ê³µ: {md_file}")
            
            # ìƒì„±ëœ MD íŒŒì¼ ë‚´ìš© í™•ì¸
            with open(md_file, 'r', encoding='utf-8') as f:
                content = f.read()
                
            if source_url in content:
                print(f"âœ… MD íŒŒì¼ì— URL í¬í•¨ë¨")
            else:
                print(f"âŒ MD íŒŒì¼ì— URL ëˆ„ë½!")
                print(f"íŒŒì¼ ë‚´ìš© ì¼ë¶€:\n{content[:500]}...")
        else:
            print("âŒ ìš”ì•½ ì‹¤íŒ¨")
    else:
        print(f"âŒ PDF íŒŒì¼ ì—†ìŒ: {pdf_path}")

def test_existing_md_files():
    """ê¸°ì¡´ MD íŒŒì¼ë“¤ì˜ URL í™•ì¸"""
    print("\nğŸ” ê¸°ì¡´ MD íŒŒì¼ URL í™•ì¸...")
    
    md_dir = Path("data/md")
    if not md_dir.exists():
        print("âŒ MD ë””ë ‰í† ë¦¬ ì—†ìŒ")
        return
    
    md_files = list(md_dir.glob("*.md"))
    print(f"ğŸ“„ MD íŒŒì¼ ìˆ˜: {len(md_files)}ê°œ")
    
    url_counts = {
        "ê¸°ë³¸_URL": 0,
        "ì„¸ë¶€_URL": 0,
        "URL_ì—†ìŒ": 0
    }
    
    for md_file in md_files[:5]:  # ì²˜ìŒ 5ê°œë§Œ í™•ì¸
        with open(md_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        if 'source_url:' in content:
            if 'view' in content or 'boardUid' in content:
                url_counts["ì„¸ë¶€_URL"] += 1
                print(f"âœ… {md_file.name}: ì„¸ë¶€ URL í¬í•¨")
            elif 'nbtnewsBU' in content:
                url_counts["ê¸°ë³¸_URL"] += 1
                print(f"âš ï¸ {md_file.name}: ê¸°ë³¸ URLë§Œ í¬í•¨")
            else:
                url_counts["URL_ì—†ìŒ"] += 1
                print(f"âŒ {md_file.name}: URL í˜•ì‹ ë¶ˆëª…")
        else:
            url_counts["URL_ì—†ìŒ"] += 1
            print(f"âŒ {md_file.name}: source_url í•„ë“œ ì—†ìŒ")
    
    print(f"\nğŸ“Š URL ìƒíƒœ ìš”ì•½:")
    for status, count in url_counts.items():
        print(f"  {status}: {count}ê°œ")

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸš€ URL ë§¤í•‘ ë””ë²„ê¹… í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
    
    # 1. í¬ë¡¤ëŸ¬ ì¶œë ¥ í…ŒìŠ¤íŠ¸
    crawler_results = test_crawler_output()
    
    # 2. ìš”ì•½ê¸° ì…ë ¥ í…ŒìŠ¤íŠ¸
    if crawler_results:
        test_summarizer_input(crawler_results)
    
    # 3. ê¸°ì¡´ MD íŒŒì¼ í™•ì¸
    test_existing_md_files()
    
    print("\nğŸ¯ ë””ë²„ê¹… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    main()